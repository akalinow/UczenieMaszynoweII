{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "## Initializing programming env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Color printing\n",
    "from termcolor import colored\n",
    "\n",
    "#General data operations library\n",
    "import math, string, glob\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import functools\n",
    "\n",
    "#The tensorflow library\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "import tensorflow  as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "#Plotting libraries\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Increase plots font size\n",
    "params = {'legend.fontsize': 'xx-large',\n",
    "          'figure.figsize': (10, 7),\n",
    "         'axes.labelsize': 'xx-large',\n",
    "         'axes.titlesize':'xx-large',\n",
    "         'xtick.labelsize':'xx-large',\n",
    "         'ytick.labelsize':'xx-large'}\n",
    "plt.rcParams.update(params) \n",
    "\n",
    "import os\n",
    "os.chdir(\"/scratch_hdd/akalinow/Zajecia/2023-2024/Lato/Uczenie_maszynowe_2/UczenieMaszynoweII/PL/\")\n",
    "\n",
    "#append path with python modules\n",
    "import importlib\n",
    "import sys\n",
    "sys.path.append(\"../modules\")\n",
    "\n",
    "#Private functions\n",
    "import plotting_functions as plf\n",
    "importlib.reload(plf);\n",
    "\n",
    "import text_functions as txt_fcn\n",
    "importlib.reload(txt_fcn);\n",
    "#Hide GPU\n",
    "#tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<br/><br/>\n",
    "<br/><br/>\n",
    "\n",
    "<h1 align=\"center\">\n",
    " Machine learning II\n",
    "</h1>\n",
    "\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "\n",
    "<h1 align=\"right\">\n",
    "Artur Kalinowski <br>\n",
    "University of Warsaw <br>\n",
    "Faculty of Physics <br>    \n",
    "</h1>\n",
    "\n",
    "There is an incomplete set of standard operations that we perform on various types of data before they are used as input to the model.\n",
    "The Keras API provides ready-made layers that perform many of these [operations](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n",
    "In this notebook, we will use several of them for different types of data: **numeric**, **text**, **images**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Numerical data\n",
    "\n",
    "### Normalization\n",
    "\n",
    "The standard operation we perform on numerical data before feeding it to the model input is normalisation.\n",
    "Normalisation makes the order of magnitude of the weights similar for all features and the weights themselves are not too large.\n",
    "\n",
    "```Python\n",
    "\n",
    "normalization = tf.keras.layers.Normalization(mean, variance) # Normalising data to mean and variance\n",
    "                                                               # by default: mean=0, variance=1\n",
    "                                                               # normalisation is carried out for each characteristic separately\n",
    "                                                               # requires the normalisation coefficients to be determined by the adapt(x) method\n",
    "normalization.adapt(x)                                                             \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Please:**.\n",
    "\n",
    "* generate a set of `(n,4)` numbers derived from a flat distribution in the ranges `[[-5,5],[-4,2],[2,2]]`.\n",
    "* print the minimum, maximum and mean values of the features in the set.\n",
    "* normalise the data to a range of `[0,1]` for each feature separately.\n",
    "* print the minimum, maximum and mean values of the features in the normalised set.\n",
    "* check whether the normalisation has worked as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 10_000\n",
    "#BEGIN_SOLUTION\n",
    "x = tf.random.uniform([n, 3])\n",
    "scales = np.array([[-5,5],[-4,2],[2,2]])\n",
    "ranges = scales[:,1] - scales[:,0]\n",
    "x = x * ranges + scales[:,0]\n",
    "print(colored(\"min =\", \"blue\"), tf.math.reduce_min(x, axis=0).numpy())\n",
    "print(colored(\"mean =\", \"blue\"), tf.math.reduce_mean(x, axis=0).numpy())\n",
    "print(colored(\"max =\", \"blue\"), tf.math.reduce_max(x, axis=0).numpy())\n",
    "print(colored(\"stddev =\", \"blue\"), tf.math.reduce_std(x, axis=0).numpy())\n",
    "normalization = tf.keras.layers.Normalization()\n",
    "normalization.adapt(x)\n",
    "x = normalization(x)\n",
    "print(colored(\"min =\", \"blue\"), tf.math.reduce_min(x, axis=0).numpy())\n",
    "print(colored(\"mean =\", \"blue\"), tf.math.reduce_mean(x, axis=0).numpy())\n",
    "print(colored(\"max =\", \"blue\"), tf.math.reduce_max(x, axis=0).numpy())\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### Discretisation \n",
    "\n",
    "Sometimes it is useful to divide numerical data into categories - **discretisation**.\n",
    "In situations where we do not need a high resolution, floating point values can be divided into `small`, `medium` and `large` or similar.\n",
    "Reducing the resolution from floating-point to a list of categories can also facilitate training.\n",
    "\n",
    "```Python\n",
    "\n",
    "discretization = tf.keras.layers.Discretization(num_bins, bin_boundaries, output_mode) \n",
    "                 # Convert a continuous variable to a discrete variable of the form:\n",
    "                 # output_mode = int - interval number (default value).\n",
    "                 #               one_hot - vector of the hot-encoding type\n",
    "                 # num_bins - number of intervals (requires calling adapt(x) method)\n",
    "                 # bin_boundaries - ranges of intervals\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Please**\n",
    "\n",
    "* discretise the data from the previous cell into 10 intervals.\n",
    "* draw a histogram of the interval numbers for **all** the characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "discretization = tf.keras.layers.Discretization(num_bins = 10)\n",
    "discretization.adapt(x)\n",
    "x = discretization(x)\n",
    "\n",
    "fig, axis = plt.subplots(1,1, figsize=(5,5))\n",
    "axis.hist(tf.reshape(shape=(-1,), tensor=x))\n",
    "axis.set_xlabel('index')\n",
    "axis.set_ylabel('counts')\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Images\n",
    "\n",
    "**Please:**\n",
    "\n",
    "* using the `tensorflow_datasets` library load the set `imagenette/160px`\n",
    "* draw a few pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "ds, ds_info = tfds.load('imagenette/160px', split='train', with_info=True)\n",
    "fig = tfds.show_examples(ds, ds_info, rows=1, cols=3);\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### Scaling\n",
    "\n",
    "The change of resolution - image scaling. Scaling requires an interpolation algorithm to be specified, allowing the\n",
    "to calculate the pixel values in the new image.\n",
    "\n",
    "```Python\n",
    "tf.keras.layers.Resizing(\n",
    "    height, width,                # width and height of the new image\n",
    "    interpolation='bilinear',     # interpolation algorithm\n",
    "    crop_to_aspect_ratio=False,   # cropping the image to achieve\n",
    "                                  # the same width/length ratio\n",
    "                                  # as in the original image\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Cropping\n",
    "\n",
    "A fragment - a `frame` - is cut out of the whole image:\n",
    "\n",
    "\n",
    "```Python\n",
    "tf.keras.layers.CenterCrop(\n",
    "    height, width              # width and height of the rectangle cropping\n",
    "                               # the frame in the middle of the image\n",
    ")\n",
    "```\n",
    "\n",
    "Clipping at a random point can be used to enrich the sample by generating random image fragments - `augmenting`. Layers performing random operations on images are only active during training by default.\n",
    "\n",
    "```Python\n",
    "tf.keras.layers.RandomCrop(\n",
    "    height, width, seed=None,  # width and height of the rectangle cropping\n",
    "                               # the frame in the random place\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "### Rotation\n",
    "\n",
    "```Python\n",
    "tf.keras.layers.RandomRotation(\n",
    "    factor,                         # range of rotation in 2pi units: (min, max)\n",
    "    fill_mode='reflect',            # algorithm to fill the space created after image rotation\n",
    "    interpolation='bilinear',\n",
    "    seed=None,\n",
    "    fill_value=0.0,                 # the value of the pixel used to fill the space created after moving the image,\n",
    "                                    # if `fill_mode=constant` is set.\n",
    ")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### Translation\n",
    "\n",
    "```Python\n",
    "tf.keras.layers.RandomTranslation(\n",
    "    height_factor,                  # relative vertical displacement factor: (min, max)\n",
    "    width_factor,                   # relative horizontal displacement coefficient: (min, max)\n",
    "    fill_mode='reflect',            # algorithm to fill the space created after image translation\n",
    "    interpolation='bilinear',\n",
    "    seed=None,\n",
    "    fill_value=0.0,                 # the value of the pixel used to fill the space created after moving the image,\n",
    "                                    # if `fill_mode=constant` is set.\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Please:**\n",
    "\n",
    "Draw random images from a set of `imagenette/160px` subjected to:\n",
    "* scaling the area to a resolution of `(160,160)`.\n",
    "* plot the resolution of the first example on the screen\n",
    "\n",
    "**Hints:**\n",
    "* use the `tf.data.Dataset.map()` method with the appropriate mapping function based on the appropriate layer.\n",
    "* note the data type in the tensor containing the processed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "ds = ds.map(lambda x: {\"image\": tf.keras.layers.Resizing(320,320, crop_to_aspect_ratio=True, dtype=tf.uint8)(x[\"image\"]), \"label\": x[\"label\"]})\n",
    "tfds.show_examples(ds, ds_info, rows=1, cols=3)\n",
    "item = next(iter(ds))\n",
    "x_res = item[\"image\"].shape[0]\n",
    "y_res = item[\"image\"].shape[1]\n",
    "print(colored(\"Resolution: \", \"blue\"), x_res, y_res)          \n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Please:**.\n",
    "\n",
    "Draw random images from a set of `imagenette/160px` subjected to:\n",
    "* clipping to a central area of size `(64,64)`.\n",
    "\n",
    "**Tips:**.\n",
    "* use the `tf.data.Dataset.map()` method with an appropriate mapping function based on `tf.keras.layers.CenterCrop`.\n",
    "* note the data type in the tensor containing the processed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "layer = tf.keras.layers.CenterCrop(64,64, dtype=tf.uint8)\n",
    "ds_randomCrop = ds.map(lambda x: {\"image\": layer(x[\"image\"]), \"label\": x[\"label\"]})\n",
    "tfds.show_examples(ds_randomCrop, ds_info, rows=1, cols=3);\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Please:**\n",
    "\n",
    "Draw random images from a set of `imagenette/160px` subjected to:\n",
    "* clipping to a random area of size `(64,64)`.\n",
    "\n",
    "**Hint:**\n",
    "* the use of layer in the definition of the lambda function will cause errors. Please try to interpret the error message and correct the code accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_73177/3522700965.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#BEGIN_SOLUTION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mds_randomCrop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_randomCrop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#END_SOLUTION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "#BEGIN_SOLUTION\n",
    "layer = tf.keras.layers.RandomCrop(64,64, dtype=tf.uint8)\n",
    "ds_randomCrop = ds.map(lambda x: {\"image\": layer(x[\"image\"]), \"label\": x[\"label\"]})\n",
    "tfds.show_examples(ds_randomCrop, ds_info, rows=1, cols=3);\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Please:**\n",
    "\n",
    "Draw random images from a set of `imagenette/160px` subjected to:\n",
    "* a random rotation in the range of $/pm/4$.\n",
    "* fill in the blanks after rotation with black color.\n",
    "\n",
    "**Hint:**\n",
    "* the use of layer in the definition of the lambda function will cause errors. Please try to interpret the error message and correct the code accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "layer = tf.keras.layers.RandomRotation(1/8.0, fill_mode='constant',  dtype=tf.uint8)\n",
    "ds_randomRotation = ds.map(lambda x: {\"image\": layer(x[\"image\"]), \"label\": x[\"label\"]})\n",
    "tfds.show_examples(ds_randomRotation, ds_info, rows=1, cols=3);\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Text data\n",
    "\n",
    "Converting text into digital form can be done in a number of ways. The two most common are:\n",
    "* **text vectorisation** - each tag (`token`) is assigned an integer, an index in the dictionary. The mapping ${\\mathrm tekst}\\leftrightarrow {\\mathrm indeks}$ is determined by the contents of the dataset. \n",
    "\n",
    "* **embedding** - each tag is assigned an n-dimensional vector of floating point numbers. The mapping ${\\mathrm tekst}\\leftrightarrow {\\mathrm indeks}$ is found during model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Vectorization\n",
    "\n",
    "```Python\n",
    "tf.keras.layers.TextVectorization(\n",
    "    max_tokens=None,                           # maximum number of tokens in the dictionary\n",
    "    standardize='lower_and_strip_punctuation', # text standardisation algorithm\n",
    "    split='whitespace',                        # word splitting algorithm\n",
    "    ngrams=None,                               # word splitting algorithm into n-letter chunks \n",
    "    output_mode='int',                         # output type  \n",
    "    output_sequence_length=None,               # maximum length of encoded 'sentence' sequence \n",
    "    pad_to_max_tokens=False,                   # whether to complete the sequence with zeros up to the maximum length\n",
    "    vocabulary=None                            # Dictionary. If not specified vocabulary generation requires calling the adapt() method\n",
    ")\n",
    " ```\n",
    "\n",
    " Tags not present in the dictionary will be given the same index as the OOV tag (`out of vocabulary`) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Please:**.\n",
    "\n",
    "* build a dictionary on the `wksf/Korpus_surowy` text loaded as part of the homework from the previous notebook.\n",
    "* vectorise the text `Król zasiada na tronie.`\n",
    "* print the vectorised form to the screen.\n",
    "* reverse engineer the text from the vectorised form.\n",
    "* repeat the procedure for the text `Ania ma małego kotka.`\n",
    "\n",
    "**Hints**: \n",
    "* the dictionary created by the `tf.keras.layers.TextVectorization` layer is obtained by the `get_vocabulary()` method.\n",
    "* a text can be created from the elements of the `words` sequence as follows:\n",
    "```Python\n",
    "sentence = \" \".join(words)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "filePath = \"../data/wksf/Korpus_surowy/\"\n",
    "dataset = txt_fcn.load_wksf_dataset(filePath)\n",
    "\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(max_tokens=1000000, output_mode = \"int\")\n",
    "vectorize_layer.adapt(dataset.batch(128))\n",
    "\n",
    "text = 'Król zasiada na tronie.'\n",
    "#text = 'Królowa zasiada na tronie.'\n",
    "#text = 'Ania ma małego kotka.'\n",
    "print(colored(\"Text:\", \"blue\"), text)\n",
    "encoded = vectorize_layer(tf.constant(text))\n",
    "print(colored(\"Encoded:\", \"blue\"), encoded.numpy())\n",
    "\n",
    "vocabulary = vectorize_layer.get_vocabulary()\n",
    "vocab_arr = np.array(vocabulary) \n",
    "decoded = \" \".join(vocab_arr[encoded.numpy()])\n",
    "print(colored(\"Decoded: \", \"blue\"), decoded)\n",
    "#END_SOLUTION\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Embedding\n",
    "\n",
    "\n",
    "```Python\n",
    "tf.keras.layers.Embedding(\n",
    "    input_dim,                          # dictionary size - number of tags (tokens)\n",
    "    output_dim,                         # representation dimension\n",
    ")\n",
    "```\n",
    "\n",
    "The embedding layer assigns a floating point value to each token.\n",
    "Such an operation can be represented by a matrix `(output_dim, input_dim)` which acts on a hot-one vector of length `(input_dim)`.\n",
    "Here `output_dim=3`:\n",
    "\n",
    "$$\n",
    "\\huge{\n",
    "\\begin{bmatrix}\n",
    "a_{0} & b_{0} & c_{0} & \\dots \\\\\n",
    "a_{1} & b_{1} & c_{1} & \\dots \\\\\n",
    "a_{2} & b_{2} & c_{2} & \\dots \\\\\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\dots \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{0} \\\\\n",
    "a_{1} \\\\\n",
    "a_{2} \n",
    "\\end{bmatrix}\n",
    "}\n",
    "$$\n",
    "The `tf.keras.layers.Enbedding()` layer performs this operation in an optimised way.\n",
    "The enbedding matrix is usually changed when training the model that contains it, so it is not a standard preprocessing layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Please:**\n",
    "\n",
    "* vectorize the text `Król zasiada na tronie.`\n",
    "* feed the vectorised form to the input of the embedding layer with `nDims = 4`\n",
    "* print both forms of the text to the screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "nTokens = len(vocabulary)\n",
    "nDims = 4 \n",
    "embedding_layer = tf.keras.layers.Embedding(nTokens, nDims)\n",
    "\n",
    "text = 'Król zasiada na tronie.'\n",
    "#text = 'Królowa zasiada na tronie.'\n",
    "encoded = vectorize_layer(tf.constant(text))\n",
    "print(colored(\"Encoded:\", \"blue\"), encoded.numpy())\n",
    "print(colored(\"Embedded: \", \"blue\"), embedding_layer(encoded).numpy())\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Division into n-grams\n",
    "\n",
    "During analysis, a text is usually divided into sections containing `n` tokens - n-grams.\n",
    "We will divide the sentences read from the Polish language corpus into segments of `n` word length. We will make use of ready-to-use functions for operating on text, available in the dedicated library `tensorflow_text`.\n",
    "\n",
    "* splitting text into fragments (here words separated by a space):\n",
    "```Python\n",
    "tensorflow_text.WhitespaceTokenizer().tokenize(text)\n",
    "```\n",
    "\n",
    "* create groups of the chosen length using a running window - groups cross over except for the last word\n",
    "```Python\n",
    "tensorflow_text.tf_text.sliding_window(data,        # token list\n",
    "                                        width,      # width of window running along list\n",
    "                                        axis=-1,    # the dimension along which the window runs\n",
    "                                        name=None   # function name\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_text as tf_text\n",
    "import functools\n",
    "\n",
    "# split lines into words\n",
    "dataset = dataset.map(tf_text.WhitespaceTokenizer().tokenize)\n",
    "\n",
    "# fix all function arguments except for the input data\n",
    "window_size = 5\n",
    "slidingWindowWithWidth = functools.partial(tf_text.sliding_window, width=window_size)\n",
    "\n",
    "# apply the sliding window to each line.\n",
    "# this will priduce a tensor of shape (n, width) for each line,\n",
    "# where n in the number of groups of words of words of width length\n",
    "dataset = dataset.map(slidingWindowWithWidth)\n",
    "\n",
    "# remove empty lines \n",
    "dataset = dataset.filter(lambda x: tf.size(x) > 0)\n",
    "\n",
    "# split the (n, width) tensor into (n) tensors of shape (width)\n",
    "dataset = dataset.unbatch()\n",
    "\n",
    "# merge words into sentence framgents\n",
    "dataset = dataset.map(lambda x: tf.strings.reduce_join(x, separator=' '))\n",
    "\n",
    "print(colored(\"First five five-word blocks:\", \"blue\"))\n",
    "for item in dataset.take(5):\n",
    "    print(colored(\"Text: \", \"blue\"), item.numpy().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please:**\n",
    "\n",
    "* create a dictionary for text vectorization using `tf.keras.layers.TextVectorization.adapt(...)` layer\n",
    "* create a vectorised dataset: `dataset_vectorized` using `tf.keras.layers.TextVectorization` layer and `dataset.map()` operation\n",
    "* store the dictionary in the `vocabulary` variable as a numpy array.\n",
    "* print the number of tokens in the dictionary to the screen.\n",
    "* print the first five examples to the screen in a vectorised form.\n",
    "\n",
    "**Hint:**\n",
    "* operations on datasets can be sped up by performing them on batches:\n",
    "```Python\n",
    "dataset.batch(n).map(...).unbatch()\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_73177/3376814412.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#BEGIN_SOLUTION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvectorize_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextVectorization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"int\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvectorize_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorize_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvocabulary_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "#BEGIN_SOLUTION\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(output_mode = \"int\")\n",
    "vectorize_layer.adapt(dataset.batch(1024))\n",
    "vocabulary = np.array(vectorize_layer.get_vocabulary())\n",
    "vocabulary_length = vocabulary.shape[0] \n",
    "dataset_vectorized = dataset.batch(1024).map(vectorize_layer, num_parallel_calls=tf.data.AUTOTUNE).unbatch()\n",
    "dataset_vectorized = dataset_vectorized.filter(lambda x: tf.math.count_nonzero(x==1, axis=0) < 2)\n",
    "print(colored(\"Vocabulary length: \", \"blue\"), vocabulary_length)\n",
    "#END_SOLUTION\n",
    "\n",
    "print(colored(\"First five five-word blocks in the vectorized form:\", \"blue\"))\n",
    "for item in dataset_vectorized.take(5):\n",
    "    print(colored(\"Text: \", \"blue\"), item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please:**\n",
    "\n",
    "* transform the vectorized set containing n-grams to the form `(features, label)` where:\n",
    "    * **label** - middle word\n",
    "    * **features** - the words outside the middle word\n",
    "* the transformation should use the `Dataset.map(...)` method using a custom mapping function `map_fn(...)`.\n",
    "* assume that the collection has been split into batches, so that a single feature has the shape `(None,width)`.\n",
    "* print the features and labels for the five examples to the screen.\n",
    "\n",
    "**Hints**: \n",
    "* it can be assumed that `n=5`.\n",
    "* the middle word can be assumed to have index `2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "def map_fn(x):\n",
    "    #BEGIN_SOLUTION\n",
    "    middle = 2\n",
    "    features =  tf.concat((x[:,:middle], x[:,middle+1:]), axis=1)\n",
    "    label = x[:,middle]\n",
    "    #END_SOLUTION\n",
    "    return features, label\n",
    "###################################################\n",
    "def print_item(batch, vocabulary, width=2):\n",
    "    batch_index = 0\n",
    "    item = (batch[0][batch_index], batch[1][batch_index])\n",
    "    features = \" \".join(vocabulary[item[0].numpy()[0:width]])\n",
    "    label = vocabulary[item[1].numpy()]   \n",
    "    print(colored(\"Features\", \"blue\"), end=\" \")\n",
    "    print(colored(\"(Label):\", \"red\"), end=\" \")\n",
    "\n",
    "    print(features, end=\" \")\n",
    "    print(colored(label,\"red\"), end=\" \")\n",
    "    features = \" \".join(vocabulary[item[0].numpy()[width:]])\n",
    "    print(features)\n",
    "################################################### \n",
    "\n",
    "dataset_final = dataset_vectorized.batch(32).map(map_fn)\n",
    "\n",
    "for item in dataset_final.take(5):\n",
    "    print_item(item, vocabulary)\n",
    "    print(colored(\"Vectorized form:\", \"blue\"), )\n",
    "    print(colored(\"Features: \", \"blue\"), item[0][0].numpy(), end=\" \")\n",
    "    print(colored(\"Label: \", \"blue\"), item[1][0].numpy())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please:**\n",
    "\n",
    "* check the reading speed of the final dataset using the `benchmark` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "tfds.benchmark(dataset_final)\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Homework\n",
    "\n",
    "**Please:**\n",
    "\n",
    "* load the text from file filePath = `shakespeare.txt'`\n",
    "\n",
    "```Python\n",
    "\n",
    "filePath = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "```\n",
    "\n",
    "* do the `preprocessing` of the text:\n",
    "    * division of the text into fragments of five words. One example in the new set should consist of one five-word example, not a group of formed by dividing a sentence into chunks of five words:\n",
    "      ```\n",
    "      \n",
    "      Features (Label): before we proceed any further\n",
    "      Features:  [128  33 123 639] Label:  1267\n",
    "      ```\n",
    "    * tokenisation with a dictionary limited to **1000** tokens\n",
    "    * division of fragments into label (middle word) and features (other words)\n",
    "* print out five examples on the screen, together with their features and the label\n",
    "* create an embedding layer with `128` dimensions.\n",
    "* output to the screen the five words closest to the word `man` in the embedding space with cosine distance:\n",
    "```Python\n",
    "cosine_similarity = tf.keras.losses.cosine_similarity(...)\n",
    "```\n",
    "* print out on the screen the five words closest to the sum of the words `mother` and `father` made in the embedding space\n",
    "\n",
    "**Hint:**\n",
    "* the largest `n` values in the list can be obtained with the `tf.math.top_k(...)` function\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Optional:**\n",
    "\n",
    "* perform embedding layer training with `128` dimensions using a continuous bag-of-words algorithm - [`Continous Bag of Words (CBOW)`](https://d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html#the-continuous-bag-of-words-cbow-model) (naive version).\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "* calculating the scalar product of the feature representation and all words of the dictionary requires the definition of a scalar product counting layer:\n",
    "```Python\n",
    "class Dot(tf.keras.Layer):\n",
    "    def call(self, x):\n",
    "        dot_product = ...\n",
    "        return dot_product\n",
    "\n",
    "```\n",
    "and using its definition in the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load text\n",
    "#BEGIN_SOLUTION\n",
    "filePath = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "dataset = txt_fcn.load_wksf_dataset(filePath)\n",
    "#END_SOLUTION\n",
    "\n",
    "# adapt vextorization layer\n",
    "#BEGIN_SOLUTION\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(max_tokens=1000, output_mode = \"int\")\n",
    "vectorize_layer.adapt(dataset.batch(128))\n",
    "#END_SOLUTION\n",
    "\n",
    "# split lines into words\n",
    "#BEGIN_SOLUTION\n",
    "dataset = dataset.map(tf_text.WhitespaceTokenizer().tokenize)\n",
    "#END_SOLUTION\n",
    "\n",
    "# fix all tf_text.sliding_window function arguments except for the input data\n",
    "#BEGIN_SOLUTION\n",
    "window_size = 5\n",
    "slidingWindowWithWidth = functools.partial(tf_text.sliding_window, width=window_size)\n",
    "#END_SOLUTION\n",
    "\n",
    "# apply the sliding window to each line.\n",
    "# this will produce a tensor of shape (n, width) for each line,\n",
    "# where n in the number of groups of words with length width\n",
    "#BEGIN_SOLUTION\n",
    "dataset = dataset.map(slidingWindowWithWidth)\n",
    "#END_SOLUTION\n",
    "\n",
    "# remove empty lines \n",
    "#BEGIN_SOLUTION\n",
    "dataset = dataset.filter(lambda x: tf.size(x) > 0)\n",
    "#END_SOLUTION\n",
    "\n",
    "# split the (n, width) tensor into (n) tensors of shape (width)\n",
    "#BEGIN_SOLUTION\n",
    "dataset = dataset.unbatch()\n",
    "#END_SOLUTION\n",
    "\n",
    "# merge words into sentence framgents\n",
    "#BEGIN_SOLUTION\n",
    "dataset = dataset.map(lambda x: tf.strings.reduce_join(x, separator=' '))\n",
    "#END_SOLUTION\n",
    "\n",
    "#Vectorize\n",
    "#BEGIN_SOLUTION\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(output_mode = \"int\")\n",
    "vectorize_layer.adapt(dataset.batch(1024))\n",
    "vocabulary = np.array(vectorize_layer.get_vocabulary())\n",
    "vocabulary_length = vocabulary.shape[0] \n",
    "dataset_vectorized = dataset.batch(1024).map(vectorize_layer, num_parallel_calls=tf.data.AUTOTUNE).unbatch()\n",
    "dataset_vectorized = dataset_vectorized.filter(lambda x: tf.math.count_nonzero(x==1, axis=0) < 2)\n",
    "print(colored(\"Vocabulary length: \", \"blue\"), vocabulary_length)\n",
    "\n",
    "dataset_final = dataset_vectorized.batch(32).map(map_fn)\n",
    "#END_SOLUTION\n",
    "\n",
    "for item in dataset_final.take(5):\n",
    "    print_item(item, vocabulary, width=2)\n",
    "    print(colored(\"Features: \", \"blue\"), item[0][0].numpy(), end=\" \")\n",
    "    print(colored(\"Label: \", \"blue\"), item[1][0].numpy())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOW model training (optional)\n",
    "#BEGIN_SOLUTION\n",
    "class Dot(tf.keras.Layer):\n",
    "    def call(self, x):\n",
    "        dot_product = tf.math.multiply(x[0], x[1])\n",
    "        dot_product = tf.math.reduce_sum(dot_product, axis=2)\n",
    "        return dot_product\n",
    "\n",
    "\n",
    "embedding_depth = 128\n",
    "input_layer = tf.keras.layers.Input(shape=(window_size-1,), dtype=tf.int32)\n",
    "embedding_layer = tf.keras.layers.Embedding(vocabulary_length, embedding_depth, name=\"embedding\")\n",
    "context_embedding = embedding_layer(input_layer)\n",
    "vocabulary_embedding = embedding_layer(tf.range(vocabulary_length))\n",
    "context_mean = tf.keras.layers.GlobalAveragePooling1D(keepdims=True)(context_embedding)\n",
    "dot_product = Dot()([context_mean, vocabulary_embedding])\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=dot_product)\n",
    "model.summary()\n",
    "tf.keras.utils.plot_model(model, 'fig_png/ML_model.png', show_shapes=True)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
    "#END_SOLUTION\n",
    "\n",
    "#Evaluate non trained model\n",
    "model.evaluate(dataset_final.take(16))\n",
    "\n",
    "#Training \n",
    "#BEGIN_SOLUTION\n",
    "nEpochs = 100\n",
    "initial_learning_rate = 2E-2\n",
    "    \n",
    "nStepsPerEpoch = 2200\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate,\n",
    "                decay_steps=nStepsPerEpoch*10,\n",
    "                decay_rate=0.95,\n",
    "                staircase=False)\n",
    "\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "callbacks = [early_stop_callback]\n",
    "           \n",
    "history = model.fit(dataset_final.skip(16).take(nStepsPerEpoch), \n",
    "                    validation_data=dataset_final.take(16),\n",
    "                    epochs=nEpochs,\n",
    "                    callbacks=callbacks, \n",
    "                    verbose=0)\n",
    "    \n",
    "model.evaluate(dataset_final.take(16))  \n",
    "txt_fcn.dump_embedding(model, vocabulary)\n",
    "plf.plotTrainHistory(history)\n",
    "\n",
    "# Print model predictions\n",
    "for batch in dataset_final.skip(16).take(5):\n",
    "    print_item(batch, vocabulary)\n",
    "    response = tf.math.argmax(model(batch[0]), axis=1)[0]\n",
    "    print(colored(\"Response:\", \"blue\"), vocabulary[response])\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeding space exploration - words similar to \"man\"\n",
    "#BEGIN_SOLUTION\n",
    "#embedding_layer = tf.keras.layers.Embedding(vocabulary_length, embedding_depth, name=\"embedding\")\n",
    "embedding_layer = model.get_layer(\"embedding\")\n",
    "vocabulary_embedding = embedding_layer(tf.range(vocabulary_length))\n",
    "\n",
    "word = \"man\"\n",
    "word_index = np.where(vocabulary == word)[0][0]\n",
    "word_embedding = vocabulary_embedding[word_index]\n",
    "\n",
    "print(colored(\"Word embedding:\", \"blue\"), word_embedding.shape)\n",
    "print(colored(\"Vocabulary embedding:\", \"blue\"), vocabulary_embedding.shape)\n",
    "cosine_similarity = -tf.keras.losses.cosine_similarity(word_embedding, vocabulary_embedding, axis=-1)\n",
    "euclidean_distance = tf.norm(word_embedding - vocabulary_embedding, axis=-1)\n",
    "\n",
    "top_k = tf.math.top_k(cosine_similarity, k=5)\n",
    "#top_k = tf.math.top_k(euclidean_distance, k=5)\n",
    "top_k_indices = top_k.indices.numpy()\n",
    "top_k_values = top_k.values.numpy() \n",
    "top_k_words = vocabulary[top_k_indices]\n",
    "print(colored(\"Top 5 words similar to: \", \"blue\"), word)\n",
    "for word, distance in zip(top_k_words, top_k_values):\n",
    "    print(colored(\"\\t\"+word+\"\\t\", \"red\"), distance)\n",
    " #END_SOLUTION\n",
    "pass   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word arithmetics - words similar to \"mother\" + \"father\"\n",
    "words = np.array([\"father\", \"mother\"])\n",
    "#BEGIN_SOLUTION\n",
    "words_indices = [np.where(vocabulary == x)[0][0] for x in words]\n",
    "words_embedding = tf.gather(vocabulary_embedding, words_indices)\n",
    "word_embedding =  words_embedding[0] + words_embedding[1]\n",
    "euclidean_distance = tf.norm(word_embedding - vocabulary_embedding, axis=-1)\n",
    "cosine_similarity =  tf.keras.losses.cosine_similarity(word_embedding, vocabulary_embedding, axis=-1)\n",
    "top_k = tf.math.top_k(-euclidean_distance, k=5)\n",
    "#top_k = tf.math.top_k(-cosine_similarity, k=5)\n",
    "top_k_indices = top_k.indices.numpy()\n",
    "top_k_values = top_k.values.numpy() \n",
    "top_k_words = vocabulary[top_k_indices]\n",
    "print(colored(\"Top words similar to: \", \"blue\"), words[1],\" + \", words[0])\n",
    "for word, distance in zip(top_k_words, top_k_values):\n",
    "    print(colored(\"\\t\"+word+\"\\t\", \"red\"), distance)\n",
    "#END_SOLUTION\n",
    "pass    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The immersion can be visualised using the portal [Embeding Projector](http://projector.tensorflow.org/?_gl=1*u2l7wh*_ga*MTg4NTM3NDUwOC4xNzA3OTg4NTU4*_ga_W0YLR4190T*MTcxNTI0MzQxOC44Ny4xLjE3MTUyNDQ5NzMuMC4wLjA.)\n",
    "The `vectors.tsv` and `metadata.tsv` files obtained from the immersion layer need to be uploaded to the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_embedding(model, vocabulary):\n",
    "  import io\n",
    "  out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "  out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "  weights = model.get_layer('embedding').get_weights()[0]\n",
    "  for index, word in enumerate(vocabulary):\n",
    "    if index == 0:\n",
    "      continue  # skip 0, it's padding.\n",
    "    vec = weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "  out_v.close()\n",
    "  out_m.close()\n",
    "\n",
    "dump_embedding(model, vocabulary)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "01_Pakiety_numpy_pandas.ipynb",
   "provenance": [
    {
     "file_id": "0BzwQ_Lscn8yDWnZVeHU1MjluWFU",
     "timestamp": 1546856440599
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "rise": {
   "center": false,
   "controls": false,
   "footer": "<h3>Letnia Szkoła<br>Fizyki 2023</h3>",
   "header": "<h1>Hello</h1>",
   "progress": "true",
   "slideNumber": "c/t",
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
