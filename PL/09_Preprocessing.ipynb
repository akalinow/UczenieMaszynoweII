{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "## Inicjalizacja środowiska programistycznego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Color printing\n",
    "from termcolor import colored\n",
    "\n",
    "#General data operations library\n",
    "import math, string, glob\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#The tensorflow library\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "import tensorflow  as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "#Plotting libraries\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Increase plots font size\n",
    "params = {'legend.fontsize': 'xx-large',\n",
    "          'figure.figsize': (10, 7),\n",
    "         'axes.labelsize': 'xx-large',\n",
    "         'axes.titlesize':'xx-large',\n",
    "         'xtick.labelsize':'xx-large',\n",
    "         'ytick.labelsize':'xx-large'}\n",
    "plt.rcParams.update(params) \n",
    "\n",
    "#append path with python modules\n",
    "import importlib\n",
    "import sys\n",
    "sys.path.append(\"../modules\")\n",
    "#sys.path.append(\"/home/akalinow/scratch/Zajecia/2023-2024/Lato/Uczenie_maszynowe_2/UczenieMaszynoweII/modules\")\n",
    "\n",
    "#Private functions\n",
    "import plotting_functions as plf\n",
    "importlib.reload(plf);\n",
    "\n",
    "import emnist_functions as emnist_fcn\n",
    "importlib.reload(emnist_fcn);\n",
    "\n",
    "import text_functions as txt_fcn\n",
    "importlib.reload(txt_fcn);\n",
    "#Hide GPU\n",
    "#tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<br/><br/>\n",
    "<br/><br/>\n",
    "\n",
    "<h1 align=\"center\">\n",
    " Uczenie maszynowe II\n",
    "</h1>\n",
    "\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "\n",
    "<h1 align=\"right\">\n",
    "Artur Kalinowski <br>\n",
    "Uniwersytet Warszawski <br>\n",
    "Wydział Fizyki <br>    \n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Istnieje, niekompletny, zbiór standardowych operacji jakie wykonujemy na różnego typu danych zanim zostaną użyte jako wejście do modelu.\n",
    "API Keras dostarcza gotowych warstw wykonujących wiele z tych [operacji](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n",
    "W tym notatniku użyjemy kilku z nich dla różnych rodzajów danych: **numerycznych**, **tekstowych**, **obrazów**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Dane numeryczne\n",
    "\n",
    "### Normalizacja\n",
    "\n",
    "Standardowa operacja, jaką wykonujemy na danych numerycznych przez podaniem ich na wejście modelu to normalizacja.\n",
    "Normalizacja powoduje że rząd wielkości wag jest podobny dla wszystkich cech, a same wagi nei są zbyt duże.\n",
    "\n",
    "```Python\n",
    "\n",
    "normalization = tf.keras.layers.Normalization(mean, variance) # Normalizacja danych do średniej mean i wariancji wariance\n",
    "                                                               # domyślnie mean=0, variance=1\n",
    "                                                               # normalizacja przebiega dla każdej cechy oddzielnie\n",
    "                                                               # wymaga ustalenia współczynników normalizacji przez metodę adapt(x)\n",
    "normalization.adapt(x)                                                             \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Proszę:**\n",
    "\n",
    "* wygenerować zbiór `(n,4)` liczb pochodzących z rozkładu płaskiego w zakresach `[[-5,5],[-4,2],[2,2]]`\n",
    "* wypisać na ekran wartości minimalne, maksymalne  i średnią cech w zbiorze\n",
    "* znormalizować dane do zakresu `[0,1]` dla każdej cechy oddzielnie\n",
    "* wypisać na ekran wartości minimalne, maksymalne  i średnią cech w znormalizowanym zbiorze\n",
    "* sprawdzić czy normalizacja zadziałała zgodnie z oczekiwaniem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 10_000\n",
    "#BEGIN_SOLUTION\n",
    "x = tf.random.uniform([n, 3])\n",
    "scales = np.array([[-5,5],[-4,2],[2,2]])\n",
    "ranges = scales[:,1] - scales[:,0]\n",
    "x = x * ranges + scales[:,0]\n",
    "print(colored(\"min =\", \"blue\"), tf.math.reduce_min(x, axis=0).numpy())\n",
    "print(colored(\"mean =\", \"blue\"), tf.math.reduce_mean(x, axis=0).numpy())\n",
    "print(colored(\"max =\", \"blue\"), tf.math.reduce_max(x, axis=0).numpy())\n",
    "print(colored(\"stddev =\", \"blue\"), tf.math.reduce_std(x, axis=0).numpy())\n",
    "normalization = tf.keras.layers.Normalization()\n",
    "normalization.adapt(x)\n",
    "x = normalization(x)\n",
    "print(colored(\"min =\", \"blue\"), tf.math.reduce_min(x, axis=0).numpy())\n",
    "print(colored(\"mean =\", \"blue\"), tf.math.reduce_mean(x, axis=0).numpy())\n",
    "print(colored(\"max =\", \"blue\"), tf.math.reduce_max(x, axis=0).numpy())\n",
    "#END_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### Dyskretyzacja \n",
    "\n",
    "Czasami użyteczne jest podział danych numerycznych na kategorie - **dyskretyzacja**.\n",
    "Np. wartości możemy podzielić na `małe`, `średnie` i `duże` jeśli nie potrzebujemy dużej rozdzielczości.\n",
    "Redukcja rozdzielczości z poziomu zmiennoprzecinkowego do listy kategorii może ułatwić trening.\n",
    "\n",
    "```Python\n",
    "\n",
    "discretization = tf.keras.layers.Discretization(num_bins, bin_boundaries, output_mode) \n",
    "                 # Zamiana zmiennej ciągłej na dyskretną w postaci:\n",
    "                 # output_mode = int - numer przedziału (wartość domyślna)\n",
    "                 #               one_hot - wektor typu kodowania gorącojedynkowego\n",
    "                 # num_bins - liczba przedziałów (wymaga zawołania metody adapt(x))\n",
    "                 # bin_boundaries - zakresy przedziałów\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Proszę:**\n",
    "\n",
    "* zdyskretyzować dane z poprzedniej komórki do 10 przedziałów\n",
    "* narysować histogram numerów przedziałów dla **wszystkich** cech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "discretization = tf.keras.layers.Discretization(num_bins = 10)\n",
    "discretization.adapt(x)\n",
    "x = discretization(x)\n",
    "\n",
    "fig, axis = plt.subplots(1,1, figsize=(5,5))\n",
    "axis.hist(tf.reshape(shape=(-1,), tensor=x))\n",
    "axis.set_xlabel('index')\n",
    "axis.set_ylabel('counts')\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Obrazy\n",
    "\n",
    "**Proszę:**\n",
    "\n",
    "* korzystając z biblioteki `tensorflow_datasets` załadować zbiór `imagenette/160px`\n",
    "* narysować kilka przykładowych rysunków"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "ds, ds_info = tfds.load('imagenette/160px', split='train', with_info=True)\n",
    "ds_resize = ds_crop.map(lambda x: {\"image\": tf.keras.layers.Resizing(160,160, crop_to_aspect_ratio=False, dtype=tf.uint8)(x[\"image\"]), \"label\": x[\"label\"]})\n",
    "fig = tfds.show_examples(ds, ds_info, rows=1, cols=3);\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### Skalowanie\n",
    "\n",
    "Zmiana rozdzielczości - skalowanie obrazu. Skalowanie wymaga podania algorytmu interpolacji, pozwalającego\n",
    "na obliczenie wartości pikseli w nowym obrazie.\n",
    "\n",
    "```Python\n",
    "tf.keras.layers.Resizing(\n",
    "    height, width,                # szerokość i wysokość nowego obrazu\n",
    "    interpolation='bilinear',     # algorytm interpolacji\n",
    "    crop_to_aspect_ratio=False,   # przycinanie obrazu w celu uzyskania\n",
    "                                  # tego samego stosunku szerokość/długość\n",
    "                                  # jak w obrazie oryginalnym\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Przycinane\n",
    "\n",
    "z całego obrazu jest wycinany fragment, `ramka`:\n",
    "\n",
    "\n",
    "```Python\n",
    "tf.keras.layers.CenterCrop(\n",
    "    height, width              # szerokość i wysokość prostokąta wycinającego \n",
    "                               # fragment w środku obrazu\n",
    ")\n",
    "```\n",
    "\n",
    "Przycinanie w losowym miejscu może być użyte do wzbogacania próbki, poprzez generację\n",
    "losowych fragmentów obrazu - ang. `augmenting`. Warstwy wykonujące losowe operacje na obrazach\n",
    "są domyślnie aktywne tylko w czasie treningu.\n",
    "\n",
    "```Python\n",
    "tf.keras.layers.RandomCrop(\n",
    "    height, width, seed=None,  # szerokość i wysokość prostokąta wycinającego \n",
    "                               # fragment w losowym miejscu\n",
    "                               #\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "### Obrót\n",
    "\n",
    "```Python\n",
    "tf.keras.layers.RandomRotation(\n",
    "    factor,                         # zakres obrotu w jednostkach 2pi: (min, max)\n",
    "    fill_mode='reflect',            # algorytm wypełnienia przestrzeni powstałej po obrocie obrazu\n",
    "    interpolation='bilinear',\n",
    "    seed=None,\n",
    "    fill_value=0.0,                 # wartość piksela użytego do wypełniania przestrzeni powstałej po przesunięciu obrazu,\n",
    "                                    # jeśli jako `fill_mode=constant`\n",
    ")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### Translacja\n",
    "\n",
    "```Python\n",
    "tf.keras.layers.RandomTranslation(\n",
    "    height_factor,                  # względny współczynnik przesunięcia w pionie: (min, max)\n",
    "    width_factor,                   # względny współczynnik przesunięcia w poziomie: (min, max)\n",
    "    fill_mode='reflect',            # algorytm wypełnienia przestrzeni powstałej po przesunięciu obrazu\n",
    "    interpolation='bilinear',\n",
    "    seed=None,\n",
    "    fill_value=0.0,                 # wartość piksela użytego do wypełniania przestrzeni powstałej po przesunięciu obrazu,\n",
    "                                    # jeśli jako `fill_mode=constant`\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Proszę:**\n",
    "\n",
    "Narysować losowe obrazy ze zbioru `imagenette/160px` poddane:\n",
    "* skalowaniu obszaru do rozdzielczości `(160,160)`\n",
    "* wypisać na ekran rozdzielczość pierwszego przykładu\n",
    "\n",
    "**Wskazówki:**\n",
    "* należy użyć metody `tf.data.Dataset.map()` z odpowiednią funkcją mapowania opartą o odpowiednią warstwę\n",
    "* uwaga na typ danych w tensorze zawierającym przetworzone obrazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "ds = ds.map(lambda x: {\"image\": tf.keras.layers.Resizing(320,320, crop_to_aspect_ratio=True, dtype=tf.uint8)(x[\"image\"]), \"label\": x[\"label\"]})\n",
    "tfds.show_examples(ds, ds_info, rows=1, cols=3)\n",
    "item = next(iter(ds))\n",
    "x_res = item[\"image\"].shape[0]\n",
    "y_res = item[\"image\"].shape[1]\n",
    "print(colored(\"Resolution: \", \"blue\"), x_res, y_res)          \n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Proszę:**\n",
    "\n",
    "Narysować losowe obrazy ze zbioru `imagenette/160px` poddane:\n",
    "* przycinaniu do obszaru centralnego o rozmiarze `(64,64)`\n",
    "\n",
    "**Wskazówki:**\n",
    "* należy użyć metody `tf.data.Dataset.map()` z odpowiednią funkcją mapowania opartą o `tf.keras.layers.CenterCrop`\n",
    "* uwaga na typ danych w tensorze zawierającym przetworzone obrazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "layer = tf.keras.layers.CenterCrop(64,64, dtype=tf.uint8)\n",
    "ds_randomCrop = ds.map(lambda x: {\"image\": layer(x[\"image\"]), \"label\": x[\"label\"]})\n",
    "tfds.show_examples(ds_randomCrop, ds_info, rows=1, cols=3);\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Proszę:**\n",
    "\n",
    "Narysować losowe obrazy ze zbioru `imagenette/160px` poddane:\n",
    "\n",
    "* losowemu przycinaniu do obszaru o rozmiarze `(64,64)`\n",
    "\n",
    "**Wskazówki:**\n",
    "* użycie warstwy w definicji funkcji lambda spowoduje błędy. Proszę spróbować zinterpretować komunikat o błędzie i odpowiednio skorygować kod.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "layer = tf.keras.layers.RandomCrop(64,64, dtype=tf.uint8)\n",
    "ds_randomCrop = ds.map(lambda x: {\"image\": layer(x[\"image\"]), \"label\": x[\"label\"]})\n",
    "tfds.show_examples(ds_randomCrop, ds_info, rows=1, cols=3);\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Proszę:**\n",
    "\n",
    "Narysować losowe obrazy ze zbioru `imagenette/160px` poddane:\n",
    "\n",
    "* losowemu obrotowi w zakresie $\\pm \\pi/4$\n",
    "* puste miejsca po obrocie proszę wypełnić kolorem czarnym\n",
    "\n",
    "**Wskazówki:**\n",
    "* użycie warstwy w definicji funkcji lambda spowoduje błędy. Proszę spróbować zinterpretować komunikat o błędzie i odpowiednio skorygować kod.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "layer = tf.keras.layers.RandomRotation(1/8.0, fill_mode='constant',  dtype=tf.uint8)\n",
    "ds_randomRotation = ds.map(lambda x: {\"image\": layer(x[\"image\"]), \"label\": x[\"label\"]})\n",
    "tfds.show_examples(ds_randomRotation, ds_info, rows=1, cols=3);\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Dane tekstowe\n",
    "\n",
    "Zamiana tekstu na postać cyfrową może być wykonana na wiele sposobów. Dwa najbardziej popularne to:\n",
    "* **wektoryzacja (ang. text vectorization)** - każdemu znacznikowi (ang. `token`) jest przypisana liczba całkowita, indeks w słowniku. \n",
    "                 Odwzrorowanie   ${\\mathrm tekst}  \\leftrightarrow {\\mathrm indeks}$ jest ustalane na podstawie zawartości zbioru danych. \n",
    "\n",
    "* **zanurzanie (ang. embedding)** - każdemu znacznikowi jest przypisany n-wymiarowy wektor liczb zmiennoprzecinkowych.\n",
    "    Odwzrorowanie   ${\\mathrm tekst}  \\leftrightarrow {\\mathrm indeks}$ jest znajdowanie w czasie treningu modelu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Wektoryzacja\n",
    "\n",
    "```Python\n",
    "tf.keras.layers.TextVectorization(\n",
    "    max_tokens=None,                           # maksymalna liczba znaczników w słowniku\n",
    "    standardize='lower_and_strip_punctuation', # algorytm standaryzacji tekstu\n",
    "    split='whitespace',                        # algorytm podziału na słowa\n",
    "    ngrams=None,                               # algorytm podziału słów na n-literowe fragmenty \n",
    "    output_mode='int',                         # typ wyjścia   \n",
    "    output_sequence_length=None,               # maksymalna długość zakodowanej sekwencji \"zdania\" \n",
    "    pad_to_max_tokens=False,                   # czy dopełniać sekwencję zerami do maksymalnej długości\n",
    "    vocabulary=None                            # słownik. Jeśli nie jest podany generacja słownika wymaga zawołania\n",
    "                                               # metody adapt()\n",
    ")\n",
    " ```\n",
    "\n",
    " Znaczniki nie występujące w słowniku otrzymają ten sam indeks oznaczający znacznik OOV (`ang. out of vocabulary`) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Proszę:**\n",
    "\n",
    "* zbudować słownik na tekście `wksf/Korpus_surowy` wczytywanym w ramach zadania domowego z poprzedniego notatnika\n",
    "* zwektoryzować teskt `Król zasiada na tronie.`\n",
    "* wypisać na ekran zwektoryzowaną postać\n",
    "* przeprowadzić operację odwrotną - z postaci zwektoryzowanej odtworzyć tekst\n",
    "* powtórzyć procedurę dla tekstu `Ania ma małego kotka.`\n",
    "\n",
    "**Wskazówki**: \n",
    "* słownik utworzony przez warstwę `tf.keras.layers.TextVectorization` uzyskujemy przez metodę `get_vocabulary()`\n",
    "* z elementów sekwencji `words` można utworzyć napis w następujący sposób:\n",
    "```Python\n",
    "sentence = \" \".join(words)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "filePath = \"../data/wksf/Korpus_surowy/\"\n",
    "dataset = txtfunc.load_wksf_dataset(filePath)\n",
    "\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(output_mode = \"int\")\n",
    "vectorize_layer.adapt(dataset.batch(128))\n",
    "\n",
    "text = 'Król zasiada na tronie.'\n",
    "#text = 'Królowa zasiada na tronie.'\n",
    "#text = 'Ania ma małego kotka.'\n",
    "print(colored(\"Text:\", \"blue\"), text)\n",
    "encoded = vectorize_layer(tf.constant(text))\n",
    "print(colored(\"Encoded:\", \"blue\"), encoded.numpy())\n",
    "\n",
    "vocabulary = vectorize_layer.get_vocabulary()\n",
    "vocab_arr = np.array(vocabulary) \n",
    "decoded = \" \".join(vocab_arr[encoded.numpy()])\n",
    "print(colored(\"Decoded: \", \"blue\"), decoded)\n",
    "#END_SOLUTION\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Zanurzanie\n",
    "\n",
    "\n",
    "```Python\n",
    "tf.keras.layers.Embedding(\n",
    "    input_dim,                          # rozmiar słownika - liczba znaczników (\"tokenów\")\n",
    "    output_dim,                         # wymiar reprezentacji  \n",
    ")\n",
    "```\n",
    "\n",
    "Warstwa zanurzająca przypisuje wartość zmiennoprzecinkową każdemu znacznikowi.\n",
    "Taką operację można reprezentować przez macierz `(output_dim, input_dim)` która działa na wektor gorącojedynkowy o długości `(input_dim)`.\n",
    "Tutaj `output_dim=3`:\n",
    "\n",
    "$$\n",
    "\\huge{\n",
    "\\begin{bmatrix}\n",
    "a_{0} & b_{0} & c_{0} & \\dots \\\\\n",
    "a_{1} & b_{1} & c_{1} & \\dots \\\\\n",
    "a_{2} & b_{2} & c_{2} & \\dots \\\\\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\dots \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{0} \\\\\n",
    "a_{1} \\\\\n",
    "a_{2} \n",
    "\\end{bmatrix}\n",
    "}\n",
    "$$\n",
    "Warstwa `tf.keras.layers.Enbedding()` realizuje tę operację w sposób zoptymalizowany.\n",
    "Macierz zanurzania jest zwykle zmieniana w trakcie treningu modelu który ją zawiera, więc nie jest to standardowa warstwa wstępnego przetwarzania.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Proszę:**\n",
    "\n",
    "* zwektoryzować teskt `Król zasiada na tronie.`\n",
    "* zwektoryzowaną postać podać na wejście warstwy zanurzającej z `nDims = 4`\n",
    "* wypisać na ekran obie postacie tekstu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "nTokens = len(vocabulary)\n",
    "nDims = 4 \n",
    "embedding_layer = tf.keras.layers.Embedding(nTokens, nDims)\n",
    "\n",
    "text = 'Król zasiada na tronie.'\n",
    "#text = 'Królowa zasiada na tronie.'\n",
    "encoded = vectorize_layer(tf.constant(text))\n",
    "print(colored(\"Encoded:\", \"blue\"), encoded.numpy())\n",
    "print(colored(\"Embedded: \", \"blue\"), embedding_layer(encoded).numpy())\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podział na n-gramy\n",
    "\n",
    "W czasie analizy tekst zwykle dzieli się na fragmenty zawierające `n` znaczników (ang. `tokens`) - n-gramy.\n",
    "Podzielimy zdania wczytane z korpusu języka polskiego na odcinki o długości n wyrazów. Skorzystamy z gotowych funkcji służących do operowania na napisach dostępnych \n",
    "w dedykowanej bibliotece `tensorflow_text`\n",
    "\n",
    "* podział tekstu na fragmenty (tutaj wyrazy oddzielone spacją):\n",
    "```Python\n",
    "tensorflow_text.WhitespaceTokenizer().tokenize(text)\n",
    "```\n",
    "\n",
    "* tworzenie grup o wybranej długości z użyciem biegnącego okna - grupy się przekrywają za wyjątkiem ostatniego wyrazu\n",
    "```Python\n",
    "tensorflow_text.tf_text.sliding_window(data,        # lista znaczników        \n",
    "                                        width,      # szerokość okna przebiegającego listę\n",
    "                                        axis=-1,    # wymiar wzdłuż którego biegnie okno\n",
    "                                        name=None   # nazwa funkcji\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mText: \u001b[0m tf.Tensor([b'Before' b'we' b'proceed' b'any' b'further,'], shape=(5,), dtype=string)\n",
      "\u001b[34mText: \u001b[0m tf.Tensor([b'we' b'proceed' b'any' b'further,' b'hear'], shape=(5,), dtype=string)\n",
      "\u001b[34mText: \u001b[0m tf.Tensor([b'proceed' b'any' b'further,' b'hear' b'me'], shape=(5,), dtype=string)\n",
      "\u001b[34mText: \u001b[0m tf.Tensor([b'any' b'further,' b'hear' b'me' b'speak.'], shape=(5,), dtype=string)\n",
      "\u001b[34mText: \u001b[0m tf.Tensor([b'You' b'are' b'all' b'resolved' b'rather'], shape=(5,), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-04 14:45:16.073578: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_text as tf_text\n",
    "import functools\n",
    "\n",
    "# fix all function arguments except for the input data\n",
    "slidingWindowWithWidth = functools.partial(tf_text.sliding_window, width=5)\n",
    "\n",
    "# split lines into words\n",
    "dataset = dataset.map(tf_text.WhitespaceTokenizer().tokenize)\n",
    "\n",
    "# apply the sliding window to each line.\n",
    "# this will priduce a tensor of shape (n, width) for each line,\n",
    "# where n in the number of groups of words of words of width length\n",
    "dataset = dataset.map(slidingWindowWithWidth)\n",
    "\n",
    "# remove empty lines \n",
    "dataset = dataset.filter(lambda x: tf.size(x) > 0)\n",
    "\n",
    "# split the (n, with) tensor into (n) tensors of shape (width)\n",
    "dataset = dataset.unbatch()\n",
    "\n",
    "for item in dataset.take(5):\n",
    "    print(colored(\"Text: \", \"blue\"), item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proszę:**\n",
    "\n",
    "* przekształcić zbiór zawierający n-gramy do postaci `(cechy, etykieta)` gdzie:\n",
    "    * **etykieta** - środkowy wyraz\n",
    "    * **cechy** - wyrazy poza wyrazem środkowym\n",
    "\n",
    "* przekształcenie powinno korzystać z metody `Dataset.map(...)` z użyciem własnej funkcji mapującej\n",
    "* założyć, że zbiór został podzielony na paczki, więc pojedynczy element ma kształt `(None,width)`\n",
    "* wypisać na ekran cechy i etykiety dla pięciu przykładów\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mFeatures: \u001b[0m tf.Tensor([[b'Before' b'we' b'any' b'further,']], shape=(1, 4), dtype=string)\n",
      "\u001b[34mLabel: \u001b[0m tf.Tensor([b'proceed'], shape=(1,), dtype=string)\n",
      "\u001b[34mFeatures: \u001b[0m tf.Tensor([[b'we' b'proceed' b'further,' b'hear']], shape=(1, 4), dtype=string)\n",
      "\u001b[34mLabel: \u001b[0m tf.Tensor([b'any'], shape=(1,), dtype=string)\n",
      "\u001b[34mFeatures: \u001b[0m tf.Tensor([[b'proceed' b'any' b'hear' b'me']], shape=(1, 4), dtype=string)\n",
      "\u001b[34mLabel: \u001b[0m tf.Tensor([b'further,'], shape=(1,), dtype=string)\n",
      "\u001b[34mFeatures: \u001b[0m tf.Tensor([[b'any' b'further,' b'me' b'speak.']], shape=(1, 4), dtype=string)\n",
      "\u001b[34mLabel: \u001b[0m tf.Tensor([b'hear'], shape=(1,), dtype=string)\n",
      "\u001b[34mFeatures: \u001b[0m tf.Tensor([[b'You' b'are' b'resolved' b'rather']], shape=(1, 4), dtype=string)\n",
      "\u001b[34mLabel: \u001b[0m tf.Tensor([b'all'], shape=(1,), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-04 14:58:15.084262: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "###################################################\n",
    "def map_fn(x):\n",
    "    middle = int(x.shape[1]/2)\n",
    "    features =  tf.concat((x[:,:middle], x[:,middle+1:]), axis=1)\n",
    "    label = x[:,middle]\n",
    "    return features, label\n",
    "###################################################\n",
    "def print_item(batch, vocabulary, width=2):\n",
    "    batch_index = 0\n",
    "    item = (batch[0][batch_index], batch[1][batch_index])\n",
    "    features = \" \".join(vocabulary[item[0].numpy()[0:width]])\n",
    "    label = vocabulary[item[1].numpy()]   \n",
    "    print(colored(\"Features\", \"blue\"), end=\" \")\n",
    "    print(colored(\"(Label):\", \"red\"), end=\" \")\n",
    "\n",
    "    print(features, end=\" \")\n",
    "    print(colored(label,\"red\"), end=\" \")\n",
    "    features = \" \".join(vocabulary[item[0].numpy()[width:]])\n",
    "    print(features)\n",
    "################################################### \n",
    "\n",
    "test = dataset.batch(1).map(map_fn)\n",
    "\n",
    "for item in test.take(5):\n",
    "    print(colored(\"Features: \", \"blue\"), item[0])\n",
    "    print(colored(\"Label: \", \"blue\"), item[1])\n",
    "    #print_item(item, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Zadanie domowe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mInput file list:\u001b[0m /home/user1/.keras/datasets/shakespeare.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-04 13:50:53.034864: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-04-04 13:50:53.218111: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mVocabulary length: \u001b[0m 12850\n",
      "\u001b[34mOriginal text: \u001b[0m First Citizen:\n",
      "\u001b[34mOriginal text: \u001b[0m wholesome, we might guess they relieved us humanely;\n",
      "\u001b[34mOriginal text: \u001b[0m What he cannot help in his nature, you account a\n",
      "\u001b[34mFeatures\u001b[0m \u001b[31m(Label):\u001b[0m proceed any \u001b[31mfurther\u001b[0m hear me\n",
      "\u001b[34mFeatures\u001b[0m \u001b[31m(Label):\u001b[0m honest enough \u001b[31mwould\u001b[0m all the\n",
      "\u001b[34mFeatures\u001b[0m \u001b[31m(Label):\u001b[0m it serves \u001b[31mmy\u001b[0m purpose i\n",
      "\n",
      "************ Summary ************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-04 13:50:53.349792: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c2615b21a0422c9e253a6efda3b7b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples/sec (First included) 22162.19 ex/sec (total: 21664 ex, 0.98 sec)\n",
      "Examples/sec (First only) 397.37 ex/sec (total: 32 ex, 0.08 sec)\n",
      "Examples/sec (First excluded) 24116.20 ex/sec (total: 21632 ex, 0.90 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-04 13:50:54.329007: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong>BenchmarkResult:</strong><br/><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>num_examples</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>first+lasts</th>\n",
       "      <td>0.977521</td>\n",
       "      <td>21664</td>\n",
       "      <td>22162.191572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>0.080530</td>\n",
       "      <td>32</td>\n",
       "      <td>397.366701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lasts</th>\n",
       "      <td>0.896991</td>\n",
       "      <td>21632</td>\n",
       "      <td>24116.197333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "BenchmarkResult(stats=             duration  num_examples           avg\n",
       "first+lasts  0.977521         21664  22162.191572\n",
       "first        0.080530            32    397.366701\n",
       "lasts        0.896991         21632  24116.197333, raw_stats=                       duration\n",
       "start_time        112033.831968\n",
       "first_batch_time  112033.912498\n",
       "end_time          112034.809488\n",
       "num_iter             676.000000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filePath = \"scratch/Zajecia/2023-2024/Lato/Uczenie_maszynowe_2/UczenieMaszynoweII/data/wksf/Korpus_surowy/\"\n",
    "fileList = glob.glob(filePath + \"/*.txt\")\n",
    "\n",
    "fileList = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "\n",
    "print(colored(\"Input file list:\", \"blue\"), fileList)\n",
    "dataset = tf.data.TextLineDataset(fileList)\n",
    "\n",
    "#Remove lines with special characters\n",
    "dataset = dataset.filter(lambda x: not tf.strings.regex_full_match(x, \".*[~].*\"))\n",
    "dataset = dataset.filter(lambda x: not tf.strings.regex_full_match(x, \".*[<].*\"))\n",
    "dataset = dataset.map(lambda x: tf.strings.regex_replace(x, \"\\[[a-zA-Z0-9*]+\\]\", \"\", replace_global=True))\n",
    "dataset = dataset.map(lambda x: tf.strings.regex_replace(x, \"\\[/\\]\", \"\", replace_global=True))\n",
    "\n",
    "#Vectorize\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(output_mode = \"int\", output_sequence_length = 10)\n",
    "vectorize_layer.adapt(dataset)\n",
    "vocabulary = np.array(vectorize_layer.get_vocabulary())\n",
    "vocabulary_length = vocabulary.shape[0] \n",
    "dataset_vectorized = dataset.batch(1024).map(vectorize_layer, num_parallel_calls=tf.data.AUTOTUNE).unbatch()\n",
    "print(colored(\"Vocabulary length: \", \"blue\"), vocabulary_length)\n",
    "\n",
    "#Remove short texts\n",
    "dataset_filtered = dataset_vectorized.filter(lambda x: tf.math.count_nonzero(x) > max_window_end)\n",
    "\n",
    "#Tokenize \n",
    "batchSize = 32\n",
    "ds_tokenized = dataset_filtered.batch(batchSize).map(map_fn)\n",
    "\n",
    "# print a few examples\n",
    "# Text\n",
    "for batch in dataset.batch(batchSize).take(3):\n",
    "    print(colored(\"Original text: \", \"blue\"), batch[0].numpy().decode(\"utf-8\"))\n",
    "\n",
    "#Features and label\n",
    "for batch in ds_tokenized.take(3):\n",
    "    print_item(batch, vocabulary)\n",
    "\n",
    "#Benchmark\n",
    "tfds.benchmark(ds_tokenized, batch_size=batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mText: \u001b[0m tf.Tensor(\n",
      "[[b'Before' b'we' b'proceed' b'any' b'further,']\n",
      " [b'we' b'proceed' b'any' b'further,' b'hear']\n",
      " [b'proceed' b'any' b'further,' b'hear' b'me']\n",
      " [b'any' b'further,' b'hear' b'me' b'speak.']], shape=(4, 5), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-04 14:14:13.272582: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_text as tf_text\n",
    "import functools\n",
    "\n",
    "slidingWindowWithWidth = functools.partial(tf_text.sliding_window, width=5)\n",
    "\n",
    "test = dataset.map(tf_text.WhitespaceTokenizer().tokenize).map(slidingWindowWithWidth).filter(lambda x: tf.size(x) > 0) \n",
    "\n",
    "for item in test.take(1):\n",
    "    print(colored(\"Text: \", \"blue\"), item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "batchSize = 32\n",
    "directory = \"/scratch_hdd/akalinow/Zajecia/2023-2024/Lato/Uczenie_maszynowe_2/UczenieMaszynoweII/data/Wikipedia_PL/\"\n",
    "directory = \"/scratch_hdd/akalinow/Zajecia/2023-2024/Lato/Uczenie_maszynowe_2/UczenieMaszynoweII/data/Wikipedia_PL/plwiki3/subsets/Countries/\"\n",
    "\n",
    "dataset = tf.keras.preprocessing.text_dataset_from_directory(directory,\n",
    "            labels=None, label_mode=None, class_names=None, batch_size=batchSize,\n",
    "            max_length=None, shuffle=False, seed=None, validation_split=None,\n",
    "            subset=None, follow_links=False)\n",
    "\n",
    "#Remove HTML tags\n",
    "regexp = \"<[^>]*>\"\n",
    "dataset = dataset.map(lambda x: tf.strings.regex_replace(x, regexp, \"\", replace_global=True))\n",
    "\n",
    "#Vectorize\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(output_mode = \"int\", max_tokens=100000)\n",
    "vectorize_layer.adapt(dataset)\n",
    "vocabulary = np.array(vectorize_layer.get_vocabulary())\n",
    "vocabulary_length = vocabulary.shape[0] \n",
    "dataset_vectorized = dataset.map(vectorize_layer, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "print(colored(\"Vocabulary length: \", \"blue\"), vocabulary_length)\n",
    "\n",
    "#Remove short texts\n",
    "dataset_filtered = dataset_vectorized.filter(lambda x: tf.shape(x)[-1] > max_window_end)\n",
    "\n",
    "#Tokenize and optimize I/O\n",
    "ds_tokenized = dataset_filtered.map(map_fn).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "#Print a few examples\n",
    "#Original text\n",
    "batch_iter = iter(dataset)\n",
    "batch = next(batch_iter)\n",
    "print(colored(\"Original text: \", \"blue\"), batch[0].numpy().decode(\"utf-8\"))\n",
    "\n",
    "#Features and label\n",
    "for item in ds_tokenized.take(10):\n",
    "    print_item(item, vocabulary)\n",
    "\n",
    "#Benchmark\n",
    "tfds.benchmark(ds_tokenized, batch_size=batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,644,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ multiply (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">12850</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │     \u001b[38;5;34m1,644,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ multiply (\u001b[38;5;33mMultiply\u001b[0m)             │ (\u001b[38;5;34m12850\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,644,800</span> (6.27 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,644,800\u001b[0m (6.27 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,644,800</span> (6.27 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,644,800\u001b[0m (6.27 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(None,), output.shape=(12850, 1, 128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39mloss, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#Evaluate non trained model\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_tokenized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#Training \u001b[39;00m\n\u001b[1;32m     23\u001b[0m nEpochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/nn.py:616\u001b[0m, in \u001b[0;36msparse_categorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `output` must be at least rank 1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    613\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    614\u001b[0m     )\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    617\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `output` must have rank (ndim) `target.ndim - 1`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    618\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    619\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    620\u001b[0m     )\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n",
      "\u001b[0;31mValueError\u001b[0m: Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(None,), output.shape=(12850, 1, 128)"
     ]
    }
   ],
   "source": [
    " #Define the model\n",
    "embedding_depth = 128\n",
    "input_layer = tf.keras.layers.Input(shape=(2*window_size,), dtype=tf.int32)\n",
    "embedding_layer = tf.keras.layers.Embedding(vocabulary_length, embedding_depth, name=\"embedding\")\n",
    "context_embedding = embedding_layer(input_layer)\n",
    "vocabulary_embedding = embedding_layer(tf.range(vocabulary_length))\n",
    "context_mean = tf.keras.layers.GlobalAveragePooling1D(keepdims=True)(context_embedding)\n",
    "dot_product = tf.keras.layers.Multiply()([context_mean, vocabulary_embedding])\n",
    "\n",
    "#dot_product = tf.math.reduce_sum(dot_product, axis=2)\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=dot_product)\n",
    "model.summary()\n",
    "#tf.keras.utils.plot_model(model, '../fig_png/ML_model.png', show_shapes=True)\n",
    "tf.keras.utils.plot_model(model, 'ML_model.png', show_shapes=True)\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
    "\n",
    "#Evaluate non trained model\n",
    "model.evaluate(ds_tokenized.take(16))\n",
    "\n",
    "#Training \n",
    "nEpochs = 10\n",
    "initial_learning_rate = 2E-2\n",
    "    \n",
    "nStepsPerEpoch = 660\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate,\n",
    "                decay_steps=nStepsPerEpoch*10,\n",
    "                decay_rate=0.95,\n",
    "                staircase=False)\n",
    "\n",
    "history = model.fit(ds_tokenized.skip(16), epochs=nEpochs, \n",
    "                    validation_data=ds_tokenized.take(16),\n",
    "                    verbose=0)\n",
    "    \n",
    "model.evaluate(ds_tokenized.take(16))    \n",
    "plf.plotTrainHistory(history)\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features and label\n",
    "for batch in ds_tokenized.skip(16).take(5):\n",
    "    print_item(batch, vocabulary)\n",
    "    response = tf.math.argmax(model(batch[0]), axis=1)[0]\n",
    "    print(colored(\"Response:\", \"blue\"), vocabulary[response])\n",
    "\n",
    "\n",
    "#Exlore embedding space\n",
    "embedding_layer = model.get_layer(\"embedding\")\n",
    "vocabulary_embedding = embedding_layer(tf.range(vocabulary_length))\n",
    "\n",
    "word = \"polska\"\n",
    "word_index = np.where(vocabulary == word)[0][0]\n",
    "word_embedding = vocabulary_embedding[word_index]\n",
    "\n",
    "print(colored(\"Word embedding:\", \"blue\"), word_embedding.shape)\n",
    "print(colored(\"Vocabulary embedding:\", \"blue\"), vocabulary_embedding.shape)\n",
    "cosine_similarity = -tf.keras.losses.cosine_similarity(word_embedding, vocabulary_embedding, axis=-1)\n",
    "\n",
    "top_k = tf.math.top_k(cosine_similarity, k=5)\n",
    "top_k_indices = top_k.indices.numpy()\n",
    "top_k_values = top_k.values.numpy() \n",
    "top_k_words = vocabulary[top_k_indices]\n",
    "print(colored(\"Top 5 words similar to: \", \"blue\"), word)\n",
    "for word, distance in zip(top_k_words, top_k_values):\n",
    "    print(colored(\"\\t\"+word+\"\\t\", \"red\"), distance)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(tf.keras.Model):\n",
    "  def __init__(self, vocab_length, embedding_depth):\n",
    "    super().__init__(self)\n",
    "    self.embedding_layer = tf.keras.layers.Embedding(vocab_length, embedding_depth, name='embedding')\n",
    "    self.output_layer = tf.keras.layers.Dense(vocab_length, activation='softmax')\n",
    "   \n",
    "  def call(self, input, training=True):\n",
    "    input_embedding = self.embedding_layer(input)\n",
    "    output = tf.reduce_mean(input_embedding, axis=1)\n",
    "    output = self.output_layer(output)\n",
    "    return output\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(2*window_size,), dtype=tf.int32)\n",
    "\n",
    "model = CBOW(vocabulary_length=vocabulary_length, embedding_depth=embedding_depth)\n",
    "batch = next(iter(ds_tokenized))\n",
    "model(batch)\n",
    "model.summary()\n",
    "tf.keras.utils.plot_model(model, 'ML_model.png', show_shapes=True)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "###Training\n",
    "nEpochs = 20\n",
    "initial_learning_rate = 1E-3\n",
    "batchSize = 32\n",
    "    \n",
    "nStepsPerEpoch = int(9984/batchSize)\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate,\n",
    "                decay_steps=nStepsPerEpoch*10,\n",
    "                decay_rate=0.95,\n",
    "                staircase=False)\n",
    "\n",
    "#run training\n",
    "history = model.fit(ds_tokenized.skip(16), epochs=nEpochs, \n",
    "                    validation_data=ds_tokenized.take(16),\n",
    "                    verbose=1)\n",
    "    \n",
    "plf.plotTrainHistory(history)\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_embedding(model, vocabulary):\n",
    "  import io\n",
    "  out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "  out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "  weights = model.get_layer('embedding').get_weights()[0]\n",
    "  for index, word in enumerate(vocabulary):\n",
    "    if index == 0:\n",
    "      continue  # skip 0, it's padding.\n",
    "    vec = weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "  out_v.close()\n",
    "  out_m.close()\n",
    "\n",
    "dump_embedding(model, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "01_Pakiety_numpy_pandas.ipynb",
   "provenance": [
    {
     "file_id": "0BzwQ_Lscn8yDWnZVeHU1MjluWFU",
     "timestamp": 1546856440599
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "rise": {
   "center": false,
   "controls": false,
   "footer": "<h3>Letnia Szkoła<br>Fizyki 2023</h3>",
   "header": "<h1>Hello</h1>",
   "progress": "true",
   "slideNumber": "c/t",
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
