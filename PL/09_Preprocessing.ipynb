{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "## Inicjalizacja środowiska programistycznego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Color printing\n",
    "from termcolor import colored\n",
    "\n",
    "#General data operations library\n",
    "import math, string, glob\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import functools\n",
    "\n",
    "#The tensorflow library\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "import tensorflow  as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "#Plotting libraries\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Increase plots font size\n",
    "params = {'legend.fontsize': 'xx-large',\n",
    "          'figure.figsize': (10, 7),\n",
    "         'axes.labelsize': 'xx-large',\n",
    "         'axes.titlesize':'xx-large',\n",
    "         'xtick.labelsize':'xx-large',\n",
    "         'ytick.labelsize':'xx-large'}\n",
    "plt.rcParams.update(params) \n",
    "\n",
    "import os\n",
    "os.chdir(\"/scratch_hdd/akalinow/Zajecia/2023-2024/Lato/Uczenie_maszynowe_2/UczenieMaszynoweII/PL/\")\n",
    "\n",
    "#append path with python modules\n",
    "import importlib\n",
    "import sys\n",
    "sys.path.append(\"../modules\")\n",
    "\n",
    "#Private functions\n",
    "import plotting_functions as plf\n",
    "importlib.reload(plf);\n",
    "\n",
    "import text_functions as txt_fcn\n",
    "importlib.reload(txt_fcn);\n",
    "#Hide GPU\n",
    "#tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<br/><br/>\n",
    "<br/><br/>\n",
    "\n",
    "<h1 align=\"center\">\n",
    " Uczenie maszynowe II\n",
    "</h1>\n",
    "\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "\n",
    "<h1 align=\"right\">\n",
    "Artur Kalinowski <br>\n",
    "Uniwersytet Warszawski <br>\n",
    "Wydział Fizyki <br>    \n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Istnieje, niekompletny, zbiór standardowych operacji jakie wykonujemy na różnego typu danych zanim zostaną użyte jako wejście do modelu.\n",
    "API Keras dostarcza gotowych warstw wykonujących wiele z tych [operacji](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n",
    "W tym notatniku użyjemy kilku z nich dla różnych rodzajów danych: **numerycznych**, **tekstowych**, **obrazów**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Dane numeryczne\n",
    "\n",
    "### Normalizacja\n",
    "\n",
    "Standardowa operacja, jaką wykonujemy na danych numerycznych przez podaniem ich na wejście modelu to normalizacja.\n",
    "Normalizacja powoduje że rząd wielkości wag jest podobny dla wszystkich cech, a same wagi nei są zbyt duże.\n",
    "\n",
    "```Python\n",
    "\n",
    "normalization = tf.keras.layers.Normalization(mean, variance) # Normalizacja danych do średniej mean i wariancji wariance\n",
    "                                                               # domyślnie mean=0, variance=1\n",
    "                                                               # normalizacja przebiega dla każdej cechy oddzielnie\n",
    "                                                               # wymaga ustalenia współczynników normalizacji przez metodę adapt(x)\n",
    "normalization.adapt(x)                                                             \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Proszę:**\n",
    "\n",
    "* wygenerować zbiór `(n,4)` liczb pochodzących z rozkładu płaskiego w zakresach `[[-5,5],[-4,2],[2,2]]`\n",
    "* wypisać na ekran wartości minimalne, maksymalne  i średnią cech w zbiorze\n",
    "* znormalizować dane do zakresu `[0,1]` dla każdej cechy oddzielnie\n",
    "* wypisać na ekran wartości minimalne, maksymalne  i średnią cech w znormalizowanym zbiorze\n",
    "* sprawdzić czy normalizacja zadziałała zgodnie z oczekiwaniem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 10_00\n",
    "#BEGIN_SOLUTION\n",
    "x = tf.random.uniform([n, 3])\n",
    "scales = np.array([[-5,5],[-4,2],[-2,2]])\n",
    "ranges = scales[:,1] - scales[:,0]\n",
    "x = x * ranges + scales[:,0]\n",
    "print(colored(\"min =\", \"blue\"), tf.math.reduce_min(x, axis=0).numpy())\n",
    "print(colored(\"mean =\", \"blue\"), tf.math.reduce_mean(x, axis=0).numpy())\n",
    "print(colored(\"max =\", \"blue\"), tf.math.reduce_max(x, axis=0).numpy())\n",
    "print(colored(\"stddev =\", \"blue\"), tf.math.reduce_std(x, axis=0).numpy())\n",
    "normalization = tf.keras.layers.Normalization()\n",
    "normalization.adapt(x)\n",
    "x = normalization(x)\n",
    "print(colored(\"min =\", \"blue\"), tf.math.reduce_min(x, axis=0).numpy())\n",
    "print(colored(\"mean =\", \"blue\"), tf.math.reduce_mean(x, axis=0).numpy())\n",
    "print(colored(\"max =\", \"blue\"), tf.math.reduce_max(x, axis=0).numpy())\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### Dyskretyzacja \n",
    "\n",
    "Czasami użyteczny jest podział danych numerycznych na kategorie - **dyskretyzacja**.\n",
    "W sytuacji, gdy nie potrzebujemy dużej rozdzielczości wartości zmiennoprzecinkowe możemy podzielić \n",
    "np. na `małe`, `średnie` i `duże`.\n",
    "Redukcja rozdzielczości z poziomu zmiennoprzecinkowego do listy kategorii może też ułatwić trening.\n",
    "\n",
    "```Python\n",
    "\n",
    "discretization = tf.keras.layers.Discretization(num_bins, bin_boundaries, output_mode) \n",
    "                 # Zamiana zmiennej ciągłej na dyskretną w postaci:\n",
    "                 # output_mode = int - numer przedziału (wartość domyślna)\n",
    "                 #               one_hot - wektor typu kodowania gorącojedynkowego\n",
    "                 # num_bins - liczba przedziałów (wymaga zawołania metody adapt(x))\n",
    "                 # bin_boundaries - zakresy przedziałów\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Proszę:**\n",
    "\n",
    "* zdyskretyzować dane z poprzedniej komórki do 10 przedziałów\n",
    "* narysować histogram numerów przedziałów dla **wszystkich** cech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "discretization = tf.keras.layers.Discretization(num_bins=10)\n",
    "discretization.adapt(x)\n",
    "y = discretization(x)\n",
    "\n",
    "fig, axis = plt.subplots(1,1, figsize=(5,5))\n",
    "counts, bins, _ = axis.hist(tf.reshape(shape=(-1,), tensor=y))\n",
    "axis.set_xlabel('index')\n",
    "axis.set_ylabel('counts')\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Obrazy\n",
    "\n",
    "**Proszę:**\n",
    "\n",
    "* korzystając z biblioteki `tensorflow_datasets` załadować zbiór `imagenette/160px`\n",
    "* narysować kilka przykładowych rysunków"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "ds, ds_info = tfds.load('imagenette/160px', split='train', with_info=True)\n",
    "fig = tfds.show_examples(ds, ds_info, rows=1, cols=3);\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### Skalowanie\n",
    "\n",
    "Zmiana rozdzielczości - skalowanie obrazu. Skalowanie wymaga podania algorytmu interpolacji, pozwalającego\n",
    "na obliczenie wartości pikseli w nowym obrazie.\n",
    "\n",
    "```Python\n",
    "tf.keras.layers.Resizing(\n",
    "    height, width,                # szerokość i wysokość nowego obrazu\n",
    "    interpolation='bilinear',     # algorytm interpolacji\n",
    "    crop_to_aspect_ratio=False,   # przycinanie obrazu w celu uzyskania\n",
    "                                  # tego samego stosunku szerokość/długość\n",
    "                                  # jak w obrazie oryginalnym\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Przycinane\n",
    "\n",
    "z całego obrazu jest wycinany fragment, `ramka`:\n",
    "\n",
    "\n",
    "```Python\n",
    "tf.keras.layers.CenterCrop(\n",
    "    height, width              # szerokość i wysokość prostokąta wycinającego \n",
    "                               # fragment w środku obrazu\n",
    ")\n",
    "```\n",
    "\n",
    "Przycinanie w losowym miejscu może być użyte do wzbogacania próbki, poprzez generację\n",
    "losowych fragmentów obrazu - ang. `augmenting`. Warstwy wykonujące losowe operacje na obrazach\n",
    "są domyślnie aktywne tylko w czasie treningu.\n",
    "\n",
    "```Python\n",
    "tf.keras.layers.RandomCrop(\n",
    "    height, width, seed=None,  # szerokość i wysokość prostokąta wycinającego \n",
    "                               # fragment w losowym miejscu\n",
    "                               #\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "### Obrót\n",
    "\n",
    "```Python\n",
    "tf.keras.layers.RandomRotation(\n",
    "    factor,                         # zakres obrotu w jednostkach 2pi: (min, max)\n",
    "    fill_mode='reflect',            # algorytm wypełnienia przestrzeni powstałej po obrocie obrazu\n",
    "    interpolation='bilinear',\n",
    "    seed=None,\n",
    "    fill_value=0.0,                 # wartość piksela użytego do wypełniania przestrzeni powstałej po przesunięciu obrazu,\n",
    "                                    # jeśli jako `fill_mode=constant`\n",
    ")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### Translacja\n",
    "\n",
    "```Python\n",
    "tf.keras.layers.RandomTranslation(\n",
    "    height_factor,                  # względny współczynnik przesunięcia w pionie: (min, max)\n",
    "    width_factor,                   # względny współczynnik przesunięcia w poziomie: (min, max)\n",
    "    fill_mode='reflect',            # algorytm wypełnienia przestrzeni powstałej po przesunięciu obrazu\n",
    "    interpolation='bilinear',\n",
    "    seed=None,\n",
    "    fill_value=0.0,                 # wartość piksela użytego do wypełniania przestrzeni powstałej po przesunięciu obrazu,\n",
    "                                    # jeśli jako `fill_mode=constant`\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Proszę:**\n",
    "\n",
    "Narysować losowe obrazy ze zbioru `imagenette/160px` poddane:\n",
    "* skalowaniu obszaru do rozdzielczości `(320,320)`\n",
    "* wypisać na ekran rozdzielczość pierwszego przykładu\n",
    "\n",
    "**Wskazówki:**\n",
    "* należy użyć metody `tf.data.Dataset.map()` z odpowiednią funkcją mapowania opartą o odpowiednią warstwę\n",
    "* uwaga na typ danych w tensorze zawierającym przetworzone obrazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "ds = ds.map(lambda x: {\"image\": tf.keras.layers.Resizing(320,320, crop_to_aspect_ratio=True, dtype=tf.uint8)(x[\"image\"]), \"label\": x[\"label\"]})\n",
    "tfds.show_examples(ds, ds_info, rows=1, cols=3)\n",
    "item = next(iter(ds))\n",
    "x_res = item[\"image\"].shape[0]\n",
    "y_res = item[\"image\"].shape[1]\n",
    "print(colored(\"Resolution: \", \"blue\"), x_res, y_res)          \n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Proszę:**\n",
    "\n",
    "Narysować losowe obrazy ze zbioru `imagenette/160px` poddane:\n",
    "* przycinaniu do obszaru centralnego o rozmiarze `(64,64)`\n",
    "\n",
    "**Wskazówki:**\n",
    "* należy użyć metody `tf.data.Dataset.map()` z odpowiednią funkcją mapowania opartą o `tf.keras.layers.CenterCrop`\n",
    "* uwaga na typ danych w tensorze zawierającym przetworzone obrazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "layer = tf.keras.layers.CenterCrop(64,64, dtype=tf.uint8)\n",
    "ds_randomCrop = ds.map(lambda x: {\"image\": layer(x[\"image\"]), \"label\": x[\"label\"]})\n",
    "tfds.show_examples(ds_randomCrop, ds_info, rows=1, cols=3);\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Proszę:**\n",
    "\n",
    "Narysować losowe obrazy ze zbioru `imagenette/160px` poddane:\n",
    "\n",
    "* losowemu przycinaniu do obszaru o rozmiarze `(64,64)`\n",
    "\n",
    "**Wskazówki:**\n",
    "* użycie warstwy w definicji funkcji lambda spowoduje błędy. Proszę spróbować zinterpretować komunikat o błędzie i odpowiednio skorygować kod.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "layer = tf.keras.layers.RandomCrop(64,64, dtype=tf.uint8)\n",
    "ds_randomCrop = ds.map(lambda x: {\"image\": layer(x[\"image\"]), \"label\": x[\"label\"]})\n",
    "tfds.show_examples(ds_randomCrop, ds_info, rows=1, cols=3);\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Proszę:**\n",
    "\n",
    "Narysować losowe obrazy ze zbioru `imagenette/160px` poddane:\n",
    "\n",
    "* losowemu obrotowi w zakresie $\\pm \\pi/4$\n",
    "* puste miejsca po obrocie proszę wypełnić kolorem czarnym\n",
    "\n",
    "**Wskazówki:**\n",
    "* użycie warstwy w definicji funkcji lambda spowoduje błędy. Proszę spróbować zinterpretować komunikat o błędzie i odpowiednio skorygować kod.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "layer = tf.keras.layers.RandomRotation(1/8.0, fill_mode='constant',  dtype=tf.uint8)\n",
    "ds_randomRotation = ds.map(lambda x: {\"image\": layer(x[\"image\"]), \"label\": x[\"label\"]})\n",
    "tfds.show_examples(ds_randomRotation, ds_info, rows=1, cols=3);\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Dane tekstowe\n",
    "\n",
    "Zamiana tekstu na postać cyfrową może być wykonana na wiele sposobów. Dwa najbardziej popularne to:\n",
    "* **wektoryzacja (ang. text vectorization)** - każdemu znacznikowi (ang. `token`) jest przypisana liczba całkowita, indeks w słowniku. \n",
    "                 Odwzorowanie   ${\\mathrm tekst}  \\leftrightarrow {\\mathrm indeks}$ jest ustalane na podstawie zawartości zbioru danych. \n",
    "\n",
    "* **zanurzanie (ang. embedding)** - każdemu znacznikowi jest przypisany n-wymiarowy wektor liczb zmiennoprzecinkowych.\n",
    "    Odwzorowanie   ${\\mathrm tekst}  \\leftrightarrow {\\mathrm indeks}$ jest znajdowanie w czasie treningu modelu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Wektoryzacja\n",
    "\n",
    "```Python\n",
    "tf.keras.layers.TextVectorization(\n",
    "    max_tokens=None,                           # maksymalna liczba znaczników w słowniku\n",
    "    standardize='lower_and_strip_punctuation', # algorytm standaryzacji tekstu\n",
    "    split='whitespace',                        # algorytm podziału na słowa\n",
    "    ngrams=None,                               # algorytm podziału słów na n-literowe fragmenty \n",
    "    output_mode='int',                         # typ wyjścia   \n",
    "    output_sequence_length=None,               # maksymalna długość zakodowanej sekwencji \"zdania\" \n",
    "    pad_to_max_tokens=False,                   # czy dopełniać sekwencję zerami do maksymalnej długości\n",
    "    vocabulary=None                            # słownik. Jeśli nie jest podany generacja słownika wymaga zawołania\n",
    "                                               # metody adapt()\n",
    ")\n",
    " ```\n",
    "\n",
    " Znaczniki nie występujące w słowniku otrzymają ten sam indeks oznaczający znacznik OOV (`ang. out of vocabulary`) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Proszę:**\n",
    "\n",
    "* zbudować słownik na tekście `wksf/Korpus_surowy` wczytywanym w ramach zadania domowego z poprzedniego notatnika\n",
    "* zwektoryzować teskt `Król zasiada na tronie.`\n",
    "* wypisać na ekran zwektoryzowaną postać\n",
    "* przeprowadzić operację odwrotną - z postaci zwektoryzowanej odtworzyć tekst\n",
    "* powtórzyć procedurę dla tekstu `Ania ma małego kotka.`\n",
    "\n",
    "**Wskazówki**: \n",
    "* słownik utworzony przez warstwę `tf.keras.layers.TextVectorization` uzyskujemy przez metodę `get_vocabulary()`\n",
    "* z elementów sekwencji `words` można utworzyć napis w następujący sposób:\n",
    "```Python\n",
    "sentence = \" \".join(words)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import text_functions as txt_fcn\n",
    "\n",
    "filePath = \"../data/wksf/Korpus_surowy/\"\n",
    "dataset = txt_fcn.load_wksf_dataset(filePath)\n",
    "\n",
    "#BEGIN_SOLUTION\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(max_tokens=1000000, output_mode = \"int\")\n",
    "vectorize_layer.adapt(dataset.batch(128))\n",
    "\n",
    "text = 'Król zasiada na tronie.'\n",
    "#text = 'Królowa zasiada na tronie.'\n",
    "#text = 'Ania ma małego kotka.'\n",
    "print(colored(\"Text:\", \"blue\"), text)\n",
    "encoded = vectorize_layer(tf.constant(text))\n",
    "print(colored(\"Encoded:\", \"blue\"), encoded.numpy())\n",
    "\n",
    "vocabulary = vectorize_layer.get_vocabulary()\n",
    "vocab_arr = np.array(vocabulary) \n",
    "decoded = \" \".join(vocab_arr[encoded.numpy()])\n",
    "print(colored(\"Decoded: \", \"blue\"), decoded)\n",
    "#END_SOLUTION\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Zanurzanie\n",
    "\n",
    "\n",
    "```Python\n",
    "tf.keras.layers.Embedding(\n",
    "    input_dim,                          # rozmiar słownika - liczba znaczników (\"tokenów\")\n",
    "    output_dim,                         # wymiar reprezentacji  \n",
    ")\n",
    "```\n",
    "\n",
    "Warstwa zanurzająca przypisuje wartość zmiennoprzecinkową każdemu znacznikowi.\n",
    "Taką operację można reprezentować przez macierz `(output_dim, input_dim)` która działa na wektor gorącojedynkowy o długości `(input_dim)`\n",
    "i produkuje reprezentację zmiennoprzecinkową o długości `output_dim`\n",
    "Tutaj `output_dim=3`:\n",
    "\n",
    "$$\n",
    "\\huge{\n",
    "\\begin{bmatrix}\n",
    "a_{0} & b_{0} & c_{0} & \\dots \\\\\n",
    "a_{1} & b_{1} & c_{1} & \\dots \\\\\n",
    "a_{2} & b_{2} & c_{2} & \\dots \\\\\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\dots \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{0} \\\\\n",
    "a_{1} \\\\\n",
    "a_{2} \n",
    "\\end{bmatrix}\n",
    "}\n",
    "$$\n",
    "Warstwa `tf.keras.layers.Enbedding()` realizuje tę operację w sposób zoptymalizowany.\n",
    "Macierz zanurzania jest zwykle zmieniana w trakcie treningu modelu który ją zawiera, więc nie jest to standardowa warstwa wstępnego przetwarzania.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Proszę:**\n",
    "\n",
    "* zwektoryzować teskt `Król zasiada na tronie.`\n",
    "* zwektoryzowaną postać podać na wejście warstwy zanurzającej z `nDims = 4`\n",
    "* wypisać na ekran obie postacie tekstu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "nTokens = len(vocabulary)\n",
    "nDims = 4 \n",
    "embedding_layer = tf.keras.layers.Embedding(nTokens, nDims)\n",
    "\n",
    "text = 'Król zasiada na tronie.'\n",
    "#text = 'Królowa zasiada na tronie.'\n",
    "encoded = vectorize_layer(tf.constant(text))\n",
    "print(colored(\"Encoded:\", \"blue\"), encoded.numpy())\n",
    "print(colored(\"Embedded: \", \"blue\"), embedding_layer(encoded).numpy())\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Podział na n-gramy\n",
    "\n",
    "W czasie analizy tekst zwykle dzieli się na fragmenty zawierające `n` znaczników (ang. `tokens`) - n-gramy.\n",
    "Podzielimy zdania wczytane z korpusu języka polskiego na odcinki o długości `n` wyrazów. Skorzystamy z gotowych funkcji służących \n",
    "do operowania na napisach dostępnych w dedykowanej bibliotece `tensorflow_text`\n",
    "\n",
    "* podział tekstu na fragmenty (tutaj wyrazy oddzielone spacją):\n",
    "```Python\n",
    "tensorflow_text.WhitespaceTokenizer().tokenize(text)\n",
    "```\n",
    "\n",
    "* tworzenie grup o wybranej długości z użyciem biegnącego okna - grupy się przekrywają za wyjątkiem ostatniego wyrazu, czyli krok okna ang. `stride`\n",
    "wynosi 1\n",
    "```Python\n",
    "tensorflow_text.tf_text.sliding_window(data,       # lista znaczników        \n",
    "                                       width,      # szerokość okna przebiegającego listę\n",
    "                                       axis=-1,    # wymiar, wzdłuż którego biegnie okno\n",
    "                                       name=None   # nazwa funkcji\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow_text as tf_text\n",
    "import functools\n",
    "\n",
    "# load the dataset\n",
    "filePath = \"../data/wksf/Korpus_surowy/\"\n",
    "dataset = txt_fcn.load_wksf_dataset(filePath)\n",
    "\n",
    "# split lines into words\n",
    "dataset = dataset.map(tf_text.WhitespaceTokenizer().tokenize)\n",
    "\n",
    "# remove empty lines \n",
    "dataset = dataset.filter(lambda x: tf.size(x) > 0)\n",
    "\n",
    "# fix all function arguments except for the input data\n",
    "window_size = 5\n",
    "slidingWindowWithWidth = functools.partial(tf_text.sliding_window, width=window_size)\n",
    "\n",
    "# apply the sliding window to each line.\n",
    "# this will priduce a tensor of shape (n, width) for each line,\n",
    "# where n in the number of groups of words of words of width length\n",
    "dataset = dataset.map(slidingWindowWithWidth)\n",
    "\n",
    "print(colored(\"First example:\", \"blue\"))\n",
    "for item in dataset.take(1):\n",
    "    print(colored(\"Text: \", \"blue\"), item.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Po podziale na n-gramy z jednego wiersza zrobiło się wiele fragmentów o długości `window_size`. Możemy je traktować jako paczki.\n",
    "Operacja rozpaczkowania, `dataset.unbatch()` z powrotem przywróci zbiór do `postaci jeden przykład na wiersz`.\n",
    "\n",
    "Rozpaczkowane wyrazy możemy z powrotem połączyć w fragmenty zdań.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split the (n, width) tensor into (n) tensors of shape (width)\n",
    "dataset = dataset.unbatch()\n",
    "\n",
    "# merge words into sentence framgents\n",
    "dataset = dataset.map(lambda x: tf.strings.reduce_join(x, separator=' '))\n",
    "\n",
    "print(colored(\"First five five-word blocks:\", \"blue\"))\n",
    "for item in dataset.take(5):\n",
    "    print(colored(\"Text: \", \"blue\"), item.numpy().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Proszę:**\n",
    "\n",
    "* stworzyć słownik do wektoryzacji tekstu używając warstwy `tf.keras.layers.TextVectorization.adapt(...)`\n",
    "* stworzyć zwektoryzowany zbiór danych: `dataset_vectorized` używając warstwy `tf.keras.layers.TextVectorization` i operacji `dataset.map()`\n",
    "* zachować słownik w zmiennej `vocabulary` w postaci macierzy numpy\n",
    "* usunąć przykłady z miej niż dwo wyrazami w zdaniu\n",
    "* wypisać na ekran liczbę znaczników w słowniku\n",
    "* wypisać na ekran pięć pierwszych przykładów w zwektoryzowanej postaci\n",
    "\n",
    "**Wskazówka:**\n",
    "* operacje na zbiorach danych można przyspieszyć wykonując je na paczkach:\n",
    "```Python\n",
    "dataset.batch(n).map(...).unbatch()\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(output_mode = \"int\")\n",
    "vectorize_layer.adapt(dataset.batch(1024))\n",
    "vocabulary = np.array(vectorize_layer.get_vocabulary())\n",
    "vocabulary_length = vocabulary.shape[0] \n",
    "dataset_vectorized = dataset.batch(1024).map(vectorize_layer, num_parallel_calls=tf.data.AUTOTUNE).unbatch()\n",
    "dataset_vectorized = dataset_vectorized.filter(lambda x: tf.math.count_nonzero(x==1, axis=0) < 2)\n",
    "print(colored(\"Vocabulary length: \", \"blue\"), vocabulary_length)\n",
    "#END_SOLUTION\n",
    "\n",
    "print(colored(\"First five five-word blocks in the vectorized form:\", \"blue\"))\n",
    "for item in dataset_vectorized.take(5):\n",
    "    print(colored(\"Text: \", \"blue\"), item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Proszę:**\n",
    "\n",
    "* przekształcić zwektoryzowany zbiór zawierający n-gramy do postaci `(cechy, etykieta)` gdzie:\n",
    "    * **etykieta** - środkowy wyraz\n",
    "    * **cechy** - wyrazy poza wyrazem środkowym\n",
    "\n",
    "* przekształcenie powinno korzystać z metody `Dataset.map(...)` z użyciem własnej funkcji mapującej `map_fn(...)`\n",
    "* założyć, że zbiór został podzielony na paczki, więc pojedynczy element ma kształt `(None,width)`\n",
    "* wypisać na ekran cechy i etykiety dla pięciu przykładów\n",
    "\n",
    "**Wskazówki**: \n",
    "* można założyć że `n=5`\n",
    "* można założyć, że środkowy wyraz ma indeks `2`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "###################################################\n",
    "def map_fn(x):\n",
    "    #BEGIN_SOLUTION\n",
    "    middle = 2\n",
    "    features =  tf.concat((x[:,:middle], x[:,middle+1:]), axis=1)\n",
    "    label = x[:,middle]\n",
    "    #END_SOLUTION\n",
    "    return features, label\n",
    "###################################################\n",
    "def print_item(batch, vocabulary, width=2):\n",
    "    batch_index = 0\n",
    "    item = (batch[0][batch_index], batch[1][batch_index])\n",
    "    features = \" \".join(vocabulary[item[0].numpy()[0:width]])\n",
    "    label = vocabulary[item[1].numpy()]   \n",
    "    print(colored(\"Features\", \"blue\"), end=\" \")\n",
    "    print(colored(\"(Label):\", \"red\"), end=\" \")\n",
    "\n",
    "    print(features, end=\" \")\n",
    "    print(colored(label,\"red\"), end=\" \")\n",
    "    features = \" \".join(vocabulary[item[0].numpy()[width:]])\n",
    "    print(features)\n",
    "################################################### \n",
    "\n",
    "dataset_final = dataset_vectorized.batch(32).map(map_fn)\n",
    "\n",
    "for item in dataset_final.take(5):\n",
    "    print_item(item, vocabulary)\n",
    "    print(colored(\"Vectorized form:\", \"blue\"), )\n",
    "    print(colored(\"Features: \", \"blue\"), item[0][0].numpy(), end=\" \")\n",
    "    print(colored(\"Label: \", \"blue\"), item[1][0].numpy())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Proszę:**\n",
    "\n",
    "* sprawdzić prędkość czytania finalnego zbioru danych korzystając z funkcji `benchmark`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "tfds.benchmark(dataset_final)\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Zadanie domowe\n",
    "\n",
    "**Proszę:**\n",
    "\n",
    "* załadować tekst z pliku filePath = `shakespeare.txt'`\n",
    "\n",
    "```Python\n",
    "\n",
    "filePath = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "```\n",
    "\n",
    "* wykonać `preprocessing` tekstu:\n",
    "    * podział tekstu na fragmenty o długości pięciu wyrazów. Jeden przykład w nowym zbiorze powinien składać się jednego 5-wyrazowego fragmentu, \n",
    "      a nie grupy fragmentów powstałej z podziału zdania na kawałki o długości pięciu wyrazów:\n",
    "      ```\n",
    "      \n",
    "      Features (Label): before we proceed any further\n",
    "      Features:  [128  33 123 639] Label:  1267\n",
    "      ```\n",
    "    * tokenizacja ze słownikiem ograniczonym do **1000** znaczników\n",
    "    * podział fragmentów na etykietę (wyraz środkowy) i cechy (pozostałe wyrazy)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "* wypisać na ekran pięć przykładów z zaznaczeniem cech i etykiety\n",
    "* stworzyć warstwę zanurzającą ze `128` wymiarami\n",
    "* wypisać na ekran pięć wyrazów najbliższych wyrazowi `man` w przestrzeni zanurzającej z odległością kosinusową:\n",
    "```Python\n",
    "cosine_similarity = tf.keras.losses.cosine_similarity(...)\n",
    "```\n",
    "* wypisać na ekran pięć słów najbliższych do sumy słów `mother` i `father` wykonanej w przestrzeni zanurzenia\n",
    "\n",
    "**Wskazówki:**\n",
    "* największe `n` wartości z listy można uzyskać funkcją `tf.math.top_k(...)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<hr>\n",
    "\n",
    "**Opcjonalnie:**\n",
    "\n",
    "* przeprowadzić trening warstwy zanurzającej ze `128` wymiarami z użyciem algorytmu ciągłego worka słów - [`Continous Bag of Words (CBOW)`](https://d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html#the-continuous-bag-of-words-cbow-model) (wersja naiwna).\n",
    "\n",
    "**Wskazówki:**\n",
    "\n",
    "* obliczenie iloczynu skalarnego reprezentacji cech i wszystkich słów słownika wymaga zdefiniowania warstwy liczącej iloczyn skalarny:\n",
    "```Python\n",
    "class Dot(tf.keras.Layer):\n",
    "    def call(self, x):\n",
    "        dot_product = ...\n",
    "        return dot_product\n",
    "\n",
    "```\n",
    "i użycia jej w definicji modelu.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load text\n",
    "#BEGIN_SOLUTION\n",
    "filePath = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "dataset = txt_fcn.load_wksf_dataset(filePath)\n",
    "#END_SOLUTION\n",
    "\n",
    "# adapt vextorization layer\n",
    "#BEGIN_SOLUTION\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(max_tokens=1000, output_mode = \"int\")\n",
    "vectorize_layer.adapt(dataset.batch(128))\n",
    "#END_SOLUTION\n",
    "\n",
    "# split lines into words\n",
    "#BEGIN_SOLUTION\n",
    "dataset = dataset.map(tf_text.WhitespaceTokenizer().tokenize)\n",
    "#END_SOLUTION\n",
    "\n",
    "# fix all tf_text.sliding_window function arguments except for the input data\n",
    "#BEGIN_SOLUTION\n",
    "window_size = 5\n",
    "slidingWindowWithWidth = functools.partial(tf_text.sliding_window, width=window_size)\n",
    "#END_SOLUTION\n",
    "\n",
    "# apply the sliding window to each line.\n",
    "# this will produce a tensor of shape (n, width) for each line,\n",
    "# where n in the number of groups of words with length width\n",
    "#BEGIN_SOLUTION\n",
    "dataset = dataset.map(slidingWindowWithWidth)\n",
    "#END_SOLUTION\n",
    "\n",
    "# remove empty lines \n",
    "#BEGIN_SOLUTION\n",
    "dataset = dataset.filter(lambda x: tf.size(x) > 0)\n",
    "#END_SOLUTION\n",
    "\n",
    "# split the (n, width) tensor into (n) tensors of shape (width)\n",
    "#BEGIN_SOLUTION\n",
    "dataset = dataset.unbatch()\n",
    "#END_SOLUTION\n",
    "\n",
    "# merge words into sentence framgents\n",
    "#BEGIN_SOLUTION\n",
    "dataset = dataset.map(lambda x: tf.strings.reduce_join(x, separator=' '))\n",
    "#END_SOLUTION\n",
    "\n",
    "#Vectorize\n",
    "#BEGIN_SOLUTION\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(output_mode = \"int\")\n",
    "vectorize_layer.adapt(dataset.batch(1024))\n",
    "vocabulary = np.array(vectorize_layer.get_vocabulary())\n",
    "vocabulary_length = vocabulary.shape[0] \n",
    "dataset_vectorized = dataset.batch(1024).map(vectorize_layer, num_parallel_calls=tf.data.AUTOTUNE).unbatch()\n",
    "dataset_vectorized = dataset_vectorized.filter(lambda x: tf.math.count_nonzero(x==1, axis=0) < 2)\n",
    "print(colored(\"Vocabulary length: \", \"blue\"), vocabulary_length)\n",
    "\n",
    "dataset_final = dataset_vectorized.batch(32).map(map_fn)\n",
    "#END_SOLUTION\n",
    "\n",
    "for item in dataset_final.take(5):\n",
    "    print_item(item, vocabulary, width=2)\n",
    "    print(colored(\"Features: \", \"blue\"), item[0][0].numpy(), end=\" \")\n",
    "    print(colored(\"Label: \", \"blue\"), item[1][0].numpy())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time\n",
    "# CBOW model training (optional)\n",
    "#BEGIN_SOLUTION\n",
    "class Dot(tf.keras.Layer):\n",
    "    def call(self, x):\n",
    "        dot_product = tf.math.multiply(x[0], x[1])\n",
    "        dot_product = tf.math.reduce_sum(dot_product, axis=2)\n",
    "        return dot_product\n",
    "\n",
    "\n",
    "embedding_depth = 128\n",
    "input_layer = tf.keras.layers.Input(shape=(window_size-1,), dtype=tf.int32)\n",
    "embedding_layer = tf.keras.layers.Embedding(vocabulary_length, embedding_depth, name=\"embedding\")\n",
    "context_embedding = embedding_layer(input_layer)\n",
    "vocabulary_embedding = embedding_layer(tf.range(vocabulary_length))\n",
    "context_mean = tf.keras.layers.GlobalAveragePooling1D(keepdims=True)(context_embedding)\n",
    "dot_product = Dot()([context_mean, vocabulary_embedding])\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=dot_product)\n",
    "model.summary()\n",
    "tf.keras.utils.plot_model(model, 'fig_png/ML_model.png', show_shapes=True)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
    "#END_SOLUTION\n",
    "\n",
    "#Evaluate non trained model\n",
    "model.evaluate(dataset_final.take(16))\n",
    "\n",
    "#Training \n",
    "#BEGIN_SOLUTION\n",
    "nEpochs = 100\n",
    "initial_learning_rate = 2E-2\n",
    "    \n",
    "nStepsPerEpoch = 2200\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate,\n",
    "                decay_steps=nStepsPerEpoch*10,\n",
    "                decay_rate=0.95,\n",
    "                staircase=False)\n",
    "\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "callbacks = [early_stop_callback]\n",
    "           \n",
    "history = model.fit(dataset_final.skip(16).take(nStepsPerEpoch), \n",
    "                    validation_data=dataset_final.take(16),\n",
    "                    epochs=nEpochs,\n",
    "                    callbacks=callbacks, \n",
    "                    verbose=0)\n",
    "    \n",
    "model.evaluate(dataset_final.take(16))  \n",
    "txt_fcn.dump_embedding(model, vocabulary)\n",
    "plf.plotTrainHistory(history)\n",
    "\n",
    "# Print model predictions\n",
    "for batch in dataset_final.skip(16).take(5):\n",
    "    print_item(batch, vocabulary)\n",
    "    response = tf.math.argmax(model(batch[0]), axis=1)[0]\n",
    "    print(colored(\"Response:\", \"blue\"), vocabulary[response])\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Embeding space exploration - words similar to \"man\"\n",
    "#BEGIN_SOLUTION\n",
    "#embedding_layer = tf.keras.layers.Embedding(vocabulary_length, embedding_depth, name=\"embedding\")\n",
    "embedding_layer = model.get_layer(\"embedding\")\n",
    "vocabulary_embedding = embedding_layer(tf.range(vocabulary_length))\n",
    "\n",
    "word = \"man\"\n",
    "word_index = np.where(vocabulary == word)[0][0]\n",
    "word_embedding = vocabulary_embedding[word_index]\n",
    "\n",
    "print(colored(\"Word embedding:\", \"blue\"), word_embedding.shape)\n",
    "print(colored(\"Vocabulary embedding:\", \"blue\"), vocabulary_embedding.shape)\n",
    "cosine_similarity = -tf.keras.losses.cosine_similarity(word_embedding, vocabulary_embedding, axis=-1)\n",
    "euclidean_distance = tf.norm(word_embedding - vocabulary_embedding, axis=-1)\n",
    "\n",
    "top_k = tf.math.top_k(cosine_similarity, k=5)\n",
    "#top_k = tf.math.top_k(euclidean_distance, k=5)\n",
    "top_k_indices = top_k.indices.numpy()\n",
    "top_k_values = top_k.values.numpy() \n",
    "top_k_words = vocabulary[top_k_indices]\n",
    "print(colored(\"Top 5 words similar to: \", \"blue\"), word)\n",
    "for word, distance in zip(top_k_words, top_k_values):\n",
    "    print(colored(\"\\t\"+word+\"\\t\", \"red\"), distance)\n",
    " #END_SOLUTION\n",
    "pass   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Word arithmetics - words similar to \"mother\" + \"father\"\n",
    "words = np.array([\"father\", \"mother\"])\n",
    "#BEGIN_SOLUTION\n",
    "words_indices = [np.where(vocabulary == x)[0][0] for x in words]\n",
    "words_embedding = tf.gather(vocabulary_embedding, words_indices)\n",
    "word_embedding =  words_embedding[0] + words_embedding[1]\n",
    "euclidean_distance = tf.norm(word_embedding - vocabulary_embedding, axis=-1)\n",
    "cosine_similarity =  tf.keras.losses.cosine_similarity(word_embedding, vocabulary_embedding, axis=-1)\n",
    "top_k = tf.math.top_k(-euclidean_distance, k=5)\n",
    "#top_k = tf.math.top_k(-cosine_similarity, k=5)\n",
    "top_k_indices = top_k.indices.numpy()\n",
    "top_k_values = top_k.values.numpy() \n",
    "top_k_words = vocabulary[top_k_indices]\n",
    "print(colored(\"Top words similar to: \", \"blue\"), words[1],\" + \", words[0])\n",
    "for word, distance in zip(top_k_words, top_k_values):\n",
    "    print(colored(\"\\t\"+word+\"\\t\", \"red\"), distance)\n",
    "#END_SOLUTION\n",
    "pass    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odwzorowanie zanurzenia można zwizualizować z użyciem portalu [Embeding Projector](http://projector.tensorflow.org/?_gl=1*u2l7wh*_ga*MTg4NTM3NDUwOC4xNzA3OTg4NTU4*_ga_W0YLR4190T*MTcxNTI0MzQxOC44Ny4xLjE3MTUyNDQ5NzMuMC4wLjA.)\n",
    "Na stronę trzeba załadować pliki `vectors.tsv` i `metadata.tsv` uzyskane z warstwy zanurzającej. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dump_embedding(model, vocabulary):\n",
    "  import io\n",
    "  out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "  out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "  weights = model.get_layer('embedding').get_weights()[0]\n",
    "  for index, word in enumerate(vocabulary):\n",
    "    if index == 0:\n",
    "      continue  # skip 0, it's padding.\n",
    "    vec = weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "  out_v.close()\n",
    "  out_m.close()\n",
    "\n",
    "dump_embedding(model, vocabulary)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "01_Pakiety_numpy_pandas.ipynb",
   "provenance": [
    {
     "file_id": "0BzwQ_Lscn8yDWnZVeHU1MjluWFU",
     "timestamp": 1546856440599
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "rise": {
   "center": false,
   "controls": false,
   "footer": "<h3>Letnia Szkoła<br>Fizyki 2023</h3>",
   "header": "<h1>Hello</h1>",
   "progress": "true",
   "slideNumber": "c/t",
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
