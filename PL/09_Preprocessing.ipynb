{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "## Inicjalizacja środowiska programistycznego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Color printing\n",
    "from termcolor import colored\n",
    "\n",
    "#General data operations library\n",
    "import math, string, glob\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#The tensorflow library\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "import tensorflow  as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "#Plotting libraries\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Increase plots font size\n",
    "params = {'legend.fontsize': 'xx-large',\n",
    "          'figure.figsize': (10, 7),\n",
    "         'axes.labelsize': 'xx-large',\n",
    "         'axes.titlesize':'xx-large',\n",
    "         'xtick.labelsize':'xx-large',\n",
    "         'ytick.labelsize':'xx-large'}\n",
    "plt.rcParams.update(params) \n",
    "\n",
    "#append path with python modules\n",
    "import importlib\n",
    "import sys\n",
    "sys.path.append(\"../modules\")\n",
    "sys.path.append(\"/home/akalinow/scratch/Zajecia/2023-2024/Lato/Uczenie_maszynowe_2/UczenieMaszynoweII/modules\")\n",
    "\n",
    "#Private functions\n",
    "import plotting_functions as plf\n",
    "importlib.reload(plf);\n",
    "\n",
    "import emnist_functions as emnist_fcn\n",
    "importlib.reload(emnist_fcn);\n",
    "#Hide GPU\n",
    "#tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<br/><br/>\n",
    "<br/><br/>\n",
    "\n",
    "<h1 align=\"center\">\n",
    " Uczenie maszynowe II\n",
    "</h1>\n",
    "\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "\n",
    "<h1 align=\"right\">\n",
    "Artur Kalinowski <br>\n",
    "Uniwersytet Warszawski <br>\n",
    "Wydział Fizyki <br>    \n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Istnieje, niekompletny, zbiór standardowych operacji jakie wykonujemy na różnego typu danych zanim zostaną użyte jako wejście do modelu.\n",
    "API Keras dostarcza gotowych warstw wykonujących wiele z tych [operacji](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n",
    "W tym notatniku użyjemy kilku z nich dla różnych rodzajów danych: **numerycznych**, **tekstowych**, **obrazów**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Dane numeryczne\n",
    "\n",
    "### Normalizacja\n",
    "\n",
    "Standardowa operacja, jaką wykonujemy na danych numerycznych przez podaniem ich na wejście modelu to normalizacja.\n",
    "Normalizacja powoduje że rząd wielkości wag jest podobny dla wszystkich cech, a same wagi nei są zbyt duże.\n",
    "\n",
    "```Python\n",
    "\n",
    "normalization = tf.keras.layers.Normalization(mean, variance) # Normalizacja danych do średniej mean i wariancji wariance\n",
    "                                                               # domyślnie mean=0, variance=1\n",
    "                                                               # normalizacja przebiega dla każdej cechy oddzielnie\n",
    "                                                               # wymaga ustalenia współczynników normalizacji przez metodę adapt(x)\n",
    "normalization.adapt(x)                                                             \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Proszę:**\n",
    "\n",
    "* wygenerować zbiór `(n,4)` liczb pochodzących z rozkładu płaskiego w zakresach `[[-5,5],[-4,2],[2,2]]`\n",
    "* wypisać na ekran wartości minimalne, maksymalne  i średnią cech w zbiorze\n",
    "* znormalizować dane do zakresu `[0,1]` dla każdej cechy oddzielnie\n",
    "* wypisać na ekran wartości minimalne, maksymalne  i średnią cech w znormalizowanym zbiorze\n",
    "* sprawdzić czy normalizacja zadziałała zgodnie z oczekiwaniem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 10_000\n",
    "#BEGIN_SOLUTION\n",
    "x = tf.random.uniform([n, 3])\n",
    "scales = np.array([[-5,5],[-4,2],[2,2]])\n",
    "ranges = scales[:,1] - scales[:,0]\n",
    "x = x * ranges + scales[:,0]\n",
    "print(colored(\"min =\", \"blue\"), tf.math.reduce_min(x, axis=0).numpy())\n",
    "print(colored(\"mean =\", \"blue\"), tf.math.reduce_mean(x, axis=0).numpy())\n",
    "print(colored(\"max =\", \"blue\"), tf.math.reduce_max(x, axis=0).numpy())\n",
    "print(colored(\"stddev =\", \"blue\"), tf.math.reduce_std(x, axis=0).numpy())\n",
    "normalization = tf.keras.layers.Normalization()\n",
    "normalization.adapt(x)\n",
    "x = normalization(x)\n",
    "print(colored(\"min =\", \"blue\"), tf.math.reduce_min(x, axis=0).numpy())\n",
    "print(colored(\"mean =\", \"blue\"), tf.math.reduce_mean(x, axis=0).numpy())\n",
    "print(colored(\"max =\", \"blue\"), tf.math.reduce_max(x, axis=0).numpy())\n",
    "#END_SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### Dyskretyzacja \n",
    "\n",
    "Czasami użyteczne jest podział danych numerycznych na kategorie - **dyskretyzacja**.\n",
    "Np. wartości możemy podzielić na `małe`, `średnie` i `duże` jeśli nie potrzebujemy dużej rozdzielczości.\n",
    "Redukcja rozdzielczości z poziomu zmiennoprzecinkowego do listy kategorii może ułatwić trening.\n",
    "\n",
    "```Python\n",
    "\n",
    "discretization = tf.keras.layers.Discretization(num_bins, bin_boundaries, output_mode) \n",
    "                 # Zamiana zmiennej ciągłej na dyskretną w postaci:\n",
    "                 # output_mode = int - numer przedziału (wartość domyślna)\n",
    "                 #               one_hot - wektor typu kodowania gorącojedynkowego\n",
    "                 # num_bins - liczba przedziałów (wymaga zawołania metody adapt(x))\n",
    "                 # bin_boundaries - zakresy przedziałów\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Proszę:**\n",
    "\n",
    "* zdyskretyzować dane z poprzedniej komórki do 10 przedziałów\n",
    "* narysować histogram numerów przedziałów dla **wszystkich** cech\n",
    "* w histogramie użyć granic przedziałów znalezionych przez `tf.keras.layers.Discretization(...)`:\n",
    "```Python\n",
    "\n",
    "discretization.bin_boundaries\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "discretization = tf.keras.layers.Discretization(num_bins = 10)\n",
    "discretization.adapt(x)\n",
    "x = discretization(x)\n",
    "\n",
    "fig, axis = plt.subplots(1,1, figsize=(5,5))\n",
    "axis.hist(tf.reshape(shape=(-1,), tensor=x), bins = discretization.bin_boundaries)\n",
    "axis.set_xlabel('index')\n",
    "axis.set_ylabel('counts')\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Dane tekstowe\n",
    "\n",
    "Zamiana tekstu na postać cyfrową może być wykonana na wiele sposobów. Dwa najbardziej popularne to:\n",
    "* **wektoryzacja (ang. text vectorization)** - każdemu znacznikowi (ang. `token`) jest przypisana liczba całkowita, indeks w słowniku. \n",
    "                 Odwzrorowanie   ${\\mathrm tekst}  \\leftrightarrow {\\mathrm indeks}$ jest ustalane na podstawie zawartości zbioru danych. \n",
    "\n",
    "* **zanurzanie (ang. embedding)** - każdemu znacznikowi jest przypisany n-wymiarowy wektor liczb zmiennoprzecinkowych.\n",
    "    Odwzrorowanie   ${\\mathrm tekst}  \\leftrightarrow {\\mathrm indeks}$ jest znajdowanie w czasie treningu modelu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Wektoryzacja\n",
    "\n",
    "```Python\n",
    "tf.keras.layers.TextVectorization(\n",
    "    max_tokens=None,                           # maksymalna liczba znaczników w słowniku\n",
    "    standardize='lower_and_strip_punctuation', # algorytm standaryzacji tekstu\n",
    "    split='whitespace',                        # algorytm podziału na słowa\n",
    "    ngrams=None,                               # algorytm podziału słów na n-literowe fragmenty \n",
    "    output_mode='int',                         # typ wyjścia   \n",
    "    output_sequence_length=None,               # maksymalna długość zakodowanej sekwencji \"zdania\" \n",
    "    pad_to_max_tokens=False,                   # czy dopełniać sekwencję zerami do maksymalnej długości\n",
    "    vocabulary=None                            # słownik. Jeśli nie jest podany generacja słownika wymaga zawołania\n",
    "                                               # metody adapt()\n",
    ")\n",
    " ```\n",
    "\n",
    " Znaczniki nie występujące w słowniku otrzymają ten sam indeks oznaczający znacznik OOV (`ang. out of vocabulary`) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Proszę:**\n",
    "\n",
    "* zbudować słownik na tekście zawartym w pliku **FIXME**\n",
    "* zwektoryzować teskt `Król zasiada na tronie.`\n",
    "* wypisać na ekran zwektoryzowaną postać\n",
    "* przeprowadzić operację odwrotną - z postaci zwektoryzowanej odtworzyć tekst\n",
    "* powtórzyć procedurę dla tekstu `Ania ma małego kotka.`\n",
    "\n",
    "**Wskazówki**: \n",
    "* słownik utworzony przez warstwę `tf.keras.layers.TextVectorization` uzyskujemy przez metodę `get_vocabulary()`\n",
    "* z elementów sekwencji `words` można utworzyć napis w następujący sposób:\n",
    "```Python\n",
    "sentence = \" \".join(words)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import glob \n",
    "filePath = \"scratch/Zajecia/2023-2024/Lato/Uczenie_maszynowe_2/UczenieMaszynoweII/data/wksf/Korpus_surowy/\"\n",
    "\n",
    "fileList = glob.glob(filePath + \"/*.txt\")\n",
    "print(fileList)\n",
    "dataset = tf.data.TextLineDataset(fileList)\n",
    "dataset = dataset.filter(lambda x: not tf.strings.regex_full_match(x, \".*[~].*\"))\n",
    "dataset = dataset.filter(lambda x: not tf.strings.regex_full_match(x, \".*[<].*\"))\n",
    "dataset = dataset.map(lambda x: tf.strings.regex_replace(x, \"\\[[0-9]+\\]\", \"\", replace_global=True))\n",
    "\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(output_mode = \"int\")\n",
    "vectorize_layer.adapt(dataset.batch(128))\n",
    "\n",
    "item = next(iter(dataset))\n",
    "\n",
    "for item in dataset.take(5):\n",
    "    print(item.numpy().decode(\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "text = 'Król zasiada na tronie.'\n",
    "#text = 'Królowa zasiada na tronie.'\n",
    "#text = 'Ania ma małego kotka.'\n",
    "encoded = vectorize_layer(tf.constant(text))\n",
    "print(colored(\"Encoded:\", \"blue\"), encoded.numpy())\n",
    "\n",
    "vocabulary = vectorize_layer.get_vocabulary()\n",
    "vocab_arr = np.array(vocabulary) \n",
    "decoded = \" \".join(vocab_arr[encoded.numpy()])\n",
    "print(colored(\"Decoded: \", \"blue\"), decoded)\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Zanurzanie\n",
    "\n",
    "\n",
    "```Python\n",
    "tf.keras.layers.Embedding(\n",
    "    input_dim,                          # rozmiar słownika - liczba znaczników (\"tokenów\")\n",
    "    output_dim,                         # wymiar reprezentacji  \n",
    ")\n",
    "```\n",
    "\n",
    "Warstwa zanurzająca przypisuje wartość zmiennoprzecinkową każdemu znacznikowi.\n",
    "Taką operację można reprezentować przez macierz `(output_dim, input_dim)` która działa na wektor gorącojedynkowy o długości `(input_dim)`.\n",
    "Tutaj `output_dim=3`:\n",
    "\n",
    "$$\n",
    "\\huge{\n",
    "\\begin{bmatrix}\n",
    "a_{0} & b_{0} & c_{0} & \\dots \\\\\n",
    "a_{1} & b_{1} & c_{1} & \\dots \\\\\n",
    "a_{2} & b_{2} & c_{2} & \\dots \\\\\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\dots \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{0} \\\\\n",
    "a_{1} \\\\\n",
    "a_{2} \n",
    "\\end{bmatrix}\n",
    "}\n",
    "$$\n",
    "Warstwa `tf.keras.layers.Enbedding()` realizuje tę operację w sposób zoptymalizowany.\n",
    "Macierz zanurzania jest zwykle zmieniana w trakcie treningu modelu który ją zawiera, więc nie jest to standardowa warstwa wstępnego przetwarzania.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Proszę:**\n",
    "\n",
    "* zwektoryzować teskt `Król zasiada na tronie.`\n",
    "* zwektoryzowaną postać podać na wejście warstwy zanurzającej z `nDims = 4`\n",
    "* wypisać na ekran obie postacie tekstu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "nTokens = len(vocabulary)\n",
    "nDims = 4 \n",
    "embedding_layer = tf.keras.layers.Embedding(nTokens, nDims)\n",
    "\n",
    "text = 'Król zasiada na tronie.'\n",
    "#text = 'Królowa zasiada na tronie.'\n",
    "encoded = vectorize_layer(tf.constant(text))\n",
    "print(colored(\"Encoded:\", \"blue\"), encoded.numpy())\n",
    "print(colored(\"Embedding layer: \", \"blue\"), embedding_layer(encoded).numpy())\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Obrazy\n",
    "\n",
    "**Proszę:**\n",
    "\n",
    "* korzystając z biblioteki `tensorflow_datasets` załadować zbiór `imagenette/160px`\n",
    "* narysować kilka przykładowych rysunków"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "ds, ds_info = tfds.load('imagenette/160px', split='train', with_info=True)\n",
    "fig = tfds.show_examples(ds, ds_info, rows=1, cols=3);\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## Przycinane\n",
    "\n",
    "z całego obrazu jest wycinany fragment, `ramka`:\n",
    "\n",
    "\n",
    "```Python\n",
    "tf.keras.layers.CenterCrop(\n",
    "    height, width              # szerokość i wysokość prostokąta wycinającego \n",
    "                               # fragment w środku obrazu\n",
    ")\n",
    "```\n",
    "\n",
    "Przycinanie w losowym miejscu może być użyte do wzbogacania próbki, poprzez generację\n",
    "losowych fragmentów obrazu - ang. `augmenting`. Warstwy wykonujące losowe operacje na obrazach\n",
    "są domyślnie aktywne tylko w czasie treningu.\n",
    "\n",
    "```Python\n",
    "tf.keras.layers.RandomCrop(\n",
    "    height, width, seed=None,  # szerokość i wysokość prostokąta wycinającego \n",
    "                               # fragment w losowym miejscu\n",
    "                               #\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### Skalowanie\n",
    "\n",
    "Zmiana rozdzielczości - skalowanie obrazu. Skalowanie wymaga podania algorytmu interpolacji, pozwalającego\n",
    "na obliczenie wartości pikseli w nowym obrazie.\n",
    "\n",
    "```Python\n",
    "tf.keras.layers.Resizing(\n",
    "    height, width,                # szerokość i wysokość nowego obrazu\n",
    "    interpolation='bilinear',     # algorytm interpolacji\n",
    "    crop_to_aspect_ratio=False,   # przycinanie obrazu w celu uzyskania\n",
    "                                  # tego samego stosunku szerokość/długość\n",
    "                                  # jak w obrazie oryginalnym\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### Translacja\n",
    "\n",
    "```Python\n",
    "tf.keras.layers.RandomTranslation(\n",
    "    height_factor,                  # względny współczynnik przesunięcia w pionie: (min, max)\n",
    "    width_factor,                   # względny współczynnik przesunięcia w poziomie: (min, max)\n",
    "    fill_mode='reflect',            # algorytm wypełnienia przestrzeni powstałej po przesunięciu obrazu\n",
    "    interpolation='bilinear',\n",
    "    seed=None,\n",
    "    fill_value=0.0,                 # wartość piksela użytego do wypełniania przestrzeni powstałej po przesunięciu obrazu,\n",
    "                                    # jeśli jako `fill_mode=constant`\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "### Obrót\n",
    "\n",
    "```Python\n",
    "tf.keras.layers.RandomRotation(\n",
    "    factor,                         # zakres obrotu w jednostkach 2pi: (min, max)\n",
    "    fill_mode='reflect',            # algorytm wypełnienia przestrzeni powstałej po obrocie obrazu\n",
    "    interpolation='bilinear',\n",
    "    seed=None,\n",
    "    fill_value=0.0,                 # wartość piksela użytego do wypełniania przestrzeni powstałej po przesunięciu obrazu,\n",
    "                                    # jeśli jako `fill_mode=constant`\n",
    ")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Proszę:**\n",
    "\n",
    "Narysować losowe obrazy ze zbioru `imagenette/160px` poddane:\n",
    "* przycinaniu do obszaru centralnego o rozmiarze `(64,64)`\n",
    "\n",
    "**Wskazówki:**\n",
    "* należy użyć metody `tf.data.Dataset.map()` z odpowiednią funkcją mapowania opartą o `tf.keras.layers.CenterCrop`\n",
    "* uwaga na typ danych w tensorze zawierającym przetworzone obrazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "ds_crop = ds.map(lambda x: {\"image\": tf.keras.layers.CenterCrop(64,64, dtype=tf.uint8)(x[\"image\"]), \"label\": x[\"label\"]})\n",
    "tfds.show_examples(ds_crop, ds_info, rows=1, cols=3);\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Proszę:**\n",
    "\n",
    "Narysować losowe obrazy ze zbioru `imagenette/160px` poddane:\n",
    "* skalowaniu przyciętego obszaru do rozdzielczości `(320,320)`\n",
    "* wypisać na ekran rozdzielczość pierwszego przykładu\n",
    "\n",
    "**Wskazówki:**\n",
    "* należy użyć metody `tf.data.Dataset.map()` z odpowiednią funkcją mapowania opartą o odpowiednią warstwę\n",
    "* uwaga na typ danych w tensorze zawierającym przetworzone obrazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "ds_crop = ds\n",
    "ds_resize = ds_crop.map(lambda x: {\"image\": tf.keras.layers.Resizing(320,320, crop_to_aspect_ratio=True, dtype=tf.uint8)(x[\"image\"]), \"label\": x[\"label\"]})\n",
    "\n",
    "tfds.show_examples(ds_resize, ds_info, rows=1, cols=3)\n",
    "item = next(iter(ds_resize))\n",
    "x_res = item[\"image\"].shape[0]\n",
    "y_res = item[\"image\"].shape[1]\n",
    "print(colored(\"Resolution: \", \"blue\"), x_res, y_res)          \n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Proszę:**\n",
    "\n",
    "Narysować losowe obrazy ze zbioru `imagenette/160px` poddane:\n",
    "\n",
    "* losowemu przycinaniu do obszaru o rozmiarze `(64,64)`\n",
    "\n",
    "**Wskazówki:**\n",
    "* użycie warstwy w definicji funkcji lambda spowoduje błędy. Proszę spróbować zinterpretować komunikat o błędzie i odpowiednio skorygować kod.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "layer = tf.keras.layers.RandomCrop(64,64, dtype=tf.uint8)\n",
    "ds_randomCrop = ds.map(lambda x: {\"image\": layer(x[\"image\"]), \"label\": x[\"label\"]})\n",
    "tfds.show_examples(ds_randomCrop, ds_info, rows=1, cols=3);\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Proszę:**\n",
    "\n",
    "Narysować losowe obrazy ze zbioru `imagenette/160px` poddane:\n",
    "\n",
    "* losowemu obrotowi w zakresie $\\pm \\pi/4$\n",
    "* puste miejsca po obrocie proszę wypełnić kolorem czarnym\n",
    "\n",
    "**Wskazówki:**\n",
    "* użycie warstwy w definicji funkcji lambda spowoduje błędy. Proszę spróbować zinterpretować komunikat o błędzie i odpowiednio skorygować kod.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "layer = tf.keras.layers.RandomRotation(1/8.0, fill_mode='constant',  dtype=tf.uint8)\n",
    "ds_randomRotation = ds.map(lambda x: {\"image\": layer(x[\"image\"]), \"label\": x[\"label\"]})\n",
    "tfds.show_examples(ds_randomRotation, ds_info, rows=1, cols=3);\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Zadanie domowe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "max_window_end = 5\n",
    "\n",
    "###################################################\n",
    "def map_fn(x):\n",
    "    start = tf.random.uniform(shape=[], maxval = max_window_end-window_size, dtype=tf.int32)\n",
    "    end = start + window_size\n",
    "    features =  tf.concat((x[:,start:end], x[:,end+1:end+1+window_size]), axis=1)\n",
    "    label = x[:,end]\n",
    "\n",
    "    return features, label\n",
    "###################################################\n",
    "def print_item(batch, vocabulary):\n",
    "    batch_index = 0\n",
    "    item = (batch[0][batch_index], batch[1][batch_index])\n",
    "    features = \" \".join(vocabulary[item[0].numpy()[0:window_size]])\n",
    "    label = vocabulary[item[1].numpy()]   \n",
    "    print(colored(\"Features\", \"blue\"), end=\" \")\n",
    "    print(colored(\"(Label):\", \"red\"), end=\" \")\n",
    "\n",
    "    print(features, end=\" \")\n",
    "    print(colored(label,\"red\"), end=\" \")\n",
    "    features = \" \".join(vocabulary[item[0].numpy()[window_size:]])\n",
    "    print(features)\n",
    "################################################### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = \"scratch/Zajecia/2023-2024/Lato/Uczenie_maszynowe_2/UczenieMaszynoweII/data/wksf/Korpus_surowy/\"\n",
    "fileList = glob.glob(filePath + \"/*.txt\")\n",
    "\n",
    "fileList = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "\n",
    "print(colored(\"Input file list:\", \"blue\"), fileList)\n",
    "dataset = tf.data.TextLineDataset(fileList)\n",
    "\n",
    "#Remove lines with special characters\n",
    "dataset = dataset.filter(lambda x: not tf.strings.regex_full_match(x, \".*[~].*\"))\n",
    "dataset = dataset.filter(lambda x: not tf.strings.regex_full_match(x, \".*[<].*\"))\n",
    "dataset = dataset.map(lambda x: tf.strings.regex_replace(x, \"\\[[a-zA-Z0-9*]+\\]\", \"\", replace_global=True))\n",
    "dataset = dataset.map(lambda x: tf.strings.regex_replace(x, \"\\[/\\]\", \"\", replace_global=True))\n",
    "\n",
    "#Vectorize\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(output_mode = \"int\", output_sequence_length = 32)\n",
    "vectorize_layer.adapt(dataset)\n",
    "vocabulary = np.array(vectorize_layer.get_vocabulary())\n",
    "vocabulary_length = vocabulary.shape[0] \n",
    "dataset_vectorized = dataset.batch(1024).map(vectorize_layer, num_parallel_calls=tf.data.AUTOTUNE).unbatch()\n",
    "print(colored(\"Vocabulary length: \", \"blue\"), vocabulary_length)\n",
    "\n",
    "#Remove short texts\n",
    "dataset_filtered = dataset_vectorized.filter(lambda x: tf.math.count_nonzero(x) > max_window_end)\n",
    "\n",
    "#Tokenize \n",
    "batchSize = 1\n",
    "ds_tokenized = dataset_filtered.batch(batchSize).map(map_fn)\n",
    "\n",
    "# print a few examples\n",
    "# Text\n",
    "for batch in dataset.batch(batchSize).take(3):\n",
    "    print(colored(\"Original text: \", \"blue\"), batch[0].numpy().decode(\"utf-8\"))\n",
    "\n",
    "#Features and label\n",
    "for batch in ds_tokenized.take(3):\n",
    "    print_item(batch, vocabulary)\n",
    "\n",
    "#Benchmark\n",
    "tfds.benchmark(ds_tokenized, batch_size=batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "batchSize = 32\n",
    "directory = \"/scratch_hdd/akalinow/Zajecia/2023-2024/Lato/Uczenie_maszynowe_2/UczenieMaszynoweII/data/Wikipedia_PL/\"\n",
    "dataset = tf.keras.preprocessing.text_dataset_from_directory(directory,\n",
    "            labels=None, label_mode=None, class_names=None, batch_size=batchSize,\n",
    "            max_length=None, shuffle=False, seed=None, validation_split=None,\n",
    "            subset=None, follow_links=False)\n",
    "\n",
    "#Remove HTML tags\n",
    "regexp = \"<[^>]*>\"\n",
    "dataset = dataset.map(lambda x: tf.strings.regex_replace(x, regexp, \"\", replace_global=True))\n",
    "\n",
    "#Vectorize\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(output_mode = \"int\", max_tokens=100000)\n",
    "vectorize_layer.adapt(dataset)\n",
    "vocabulary = np.array(vectorize_layer.get_vocabulary())\n",
    "vocabulary_length = vocabulary.shape[0] \n",
    "dataset_vectorized = dataset.map(vectorize_layer, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "print(colored(\"Vocabulary length: \", \"blue\"), vocabulary_length)\n",
    "\n",
    "#Remove short texts\n",
    "dataset_filtered = dataset_vectorized.filter(lambda x: tf.shape(x)[-1] > max_window_end)\n",
    "\n",
    "#Tokenize and optimize I/O\n",
    "ds_tokenized = dataset_filtered.map(map_fn).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "#Print a few examples\n",
    "#Original text\n",
    "batch_iter = iter(dataset)\n",
    "batch = next(batch_iter)\n",
    "print(colored(\"Original text: \", \"blue\"), batch[0].numpy().decode(\"utf-8\"))\n",
    "\n",
    "#Features and label\n",
    "for item in ds_tokenized.take(10):\n",
    "    print(item[0].numpy(), item[1].numpy()) \n",
    "    print_item(item, vocabulary)\n",
    "\n",
    "#Benchmark\n",
    "tfds.benchmark(ds_tokenized, batch_size=batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Define the model\n",
    "embedding_depth = 128\n",
    "input_layer = tf.keras.layers.Input(shape=(2*window_size,), dtype=tf.int32)\n",
    "vocabulary_embedding = tf.keras.layers.Embedding(vocabulary_length, embedding_depth, name='embedding')(tf.range(vocabulary_length))\n",
    "context_embedding = tf.gather(params=vocabulary_embedding, indices=input_layer)\n",
    "context_mean = tf.math.reduce_mean(context_embedding, axis=1)\n",
    "dot_product =  tf.keras.layers.Multiply()([context_mean, vocabulary_embedding])\n",
    "dot_product = tf.math.reduce_sum(dot_product, axis=1)\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=dot_product)\n",
    "model.summary()\n",
    "#tf.keras.utils.plot_model(model, '../fig_png/ML_model.png', show_shapes=True)\n",
    "tf.keras.utils.plot_model(model, 'ML_model.png', show_shapes=True)\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
    "\n",
    "#Evaluate non trained model\n",
    "model.evaluate(ds_tokenized.take(32))\n",
    "\n",
    "#Training \n",
    "nEpochs = 30\n",
    "initial_learning_rate = 2E-2\n",
    "    \n",
    "nStepsPerEpoch = int(9984/batchSize)\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate,\n",
    "                decay_steps=nStepsPerEpoch*100,\n",
    "                decay_rate=0.95,\n",
    "                staircase=False)\n",
    "\n",
    "history = model.fit(ds_tokenized.skip(16), epochs=nEpochs, \n",
    "                    validation_data=ds_tokenized.take(16),\n",
    "                    verbose=1)\n",
    "    \n",
    "#model.evaluate(ds_tokenized.take(32))    \n",
    "plf.plotTrainHistory(history)\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.0/vocabulary_embedding.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(tf.keras.Model):\n",
    "  def __init__(self, vocab_length, embedding_depth):\n",
    "    super().__init__(self)\n",
    "    self.embedding_layer = tf.keras.layers.Embedding(vocab_length, embedding_depth, name='embedding')\n",
    "    self.output_layer = tf.keras.layers.Dense(vocab_length, activation='softmax')\n",
    "   \n",
    "  def call(self, input, training=True):\n",
    "    input_embedding = self.embedding_layer(input)\n",
    "    output = tf.reduce_mean(input_embedding, axis=1)\n",
    "    output = self.output_layer(output)\n",
    "    return output\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(2*window_size,), dtype=tf.int32)\n",
    "\n",
    "model = CBOW(vocabulary_length=vocabulary_length, embedding_depth=embedding_depth)\n",
    "batch = next(iter(ds_tokenized))\n",
    "model(batch)\n",
    "model.summary()\n",
    "tf.keras.utils.plot_model(model, 'ML_model.png', show_shapes=True)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "###Training\n",
    "nEpochs = 20\n",
    "initial_learning_rate = 1E-3\n",
    "batchSize = 32\n",
    "    \n",
    "nStepsPerEpoch = int(9984/batchSize)\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate,\n",
    "                decay_steps=nStepsPerEpoch*10,\n",
    "                decay_rate=0.95,\n",
    "                staircase=False)\n",
    "\n",
    "#run training\n",
    "history = model.fit(ds_tokenized.skip(16), epochs=nEpochs, \n",
    "                    validation_data=ds_tokenized.take(16),\n",
    "                    verbose=1)\n",
    "    \n",
    "plf.plotTrainHistory(history)\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_embedding(model, vocabulary):\n",
    "  import io\n",
    "  out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "  out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "  weights = model.get_layer('embedding').get_weights()[0]\n",
    "  for index, word in enumerate(vocabulary):\n",
    "    if index == 0:\n",
    "      continue  # skip 0, it's padding.\n",
    "    vec = weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "  out_v.close()\n",
    "  out_m.close()\n",
    "\n",
    "dump_embedding(model, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "01_Pakiety_numpy_pandas.ipynb",
   "provenance": [
    {
     "file_id": "0BzwQ_Lscn8yDWnZVeHU1MjluWFU",
     "timestamp": 1546856440599
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "rise": {
   "center": false,
   "controls": false,
   "footer": "<h3>Letnia Szkoła<br>Fizyki 2023</h3>",
   "header": "<h1>Hello</h1>",
   "progress": "true",
   "slideNumber": "c/t",
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
